<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Eigenvalue problems · KrylovKit.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">KrylovKit.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../intro/">Introduction</a></li><li><a class="tocitem" href="../linear/">Linear problems</a></li><li class="is-active"><a class="tocitem" href>Eigenvalue problems</a><ul class="internal"><li><a class="tocitem" href="#Eigenvalues-and-eigenvectors"><span>Eigenvalues and eigenvectors</span></a></li><li><a class="tocitem" href="#Generalized-eigenvalue-problems"><span>Generalized eigenvalue problems</span></a></li></ul></li><li><a class="tocitem" href="../svd/">Singular value problems</a></li><li><a class="tocitem" href="../matfun/">Functions of matrices and linear maps</a></li><li><a class="tocitem" href="../algorithms/">Available algorithms</a></li><li><a class="tocitem" href="../implementation/">Details of the implementation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Eigenvalue problems</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Eigenvalue problems</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Jutho/KrylovKit.jl/blob/master/docs/src/man/eig.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Eigenvalue-problems"><a class="docs-heading-anchor" href="#Eigenvalue-problems">Eigenvalue problems</a><a id="Eigenvalue-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Eigenvalue-problems" title="Permalink"></a></h1><h2 id="Eigenvalues-and-eigenvectors"><a class="docs-heading-anchor" href="#Eigenvalues-and-eigenvectors">Eigenvalues and eigenvectors</a><a id="Eigenvalues-and-eigenvectors-1"></a><a class="docs-heading-anchor-permalink" href="#Eigenvalues-and-eigenvectors" title="Permalink"></a></h2><p>Finding a selection of eigenvalues and corresponding (right) eigenvectors of a linear map can be accomplished with the <code>eigsolve</code> routine:</p><article class="docstring"><header><a class="docstring-binding" id="KrylovKit.eigsolve" href="#KrylovKit.eigsolve"><code>KrylovKit.eigsolve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">eigsolve(A::AbstractMatrix, [howmany = 1, which = :LM, T = eltype(A)]; kwargs...)
eigsolve(f, n::Int, [howmany = 1, which = :LM, T = Float64]; kwargs...)
eigsolve(f, x₀, [howmany = 1, which = :LM]; kwargs...)
eigsolve(f, x₀, howmany, which, algorithm)</code></pre><p>Compute at least <code>howmany</code> eigenvalues from the linear map encoded in the matrix <code>A</code> or by the function <code>f</code>. Return eigenvalues, eigenvectors and a <code>ConvergenceInfo</code> structure.</p><p><strong>Arguments:</strong></p><p>The linear map can be an <code>AbstractMatrix</code> (dense or sparse) or a general function or callable object. If an <code>AbstractMatrix</code> is used, a starting vector <code>x₀</code> does not need to be provided, it is then chosen as <code>rand(T, size(A,1))</code>. If the linear map is encoded more generally as a a callable function or method, the best approach is to provide an explicit starting guess <code>x₀</code>. Note that <code>x₀</code> does not need to be of type <code>AbstractVector</code>, any type that behaves as a vector and supports the required methods (see KrylovKit docs) is accepted. If instead of <code>x₀</code> an integer <code>n</code> is specified, it is assumed that <code>x₀</code> is a regular vector and it is initialized to <code>rand(T,n)</code>, where the default value of <code>T</code> is <code>Float64</code>, unless specified differently.</p><p>The next arguments are optional, but should typically be specified. <code>howmany</code> specifies how many eigenvalues should be computed; <code>which</code> specifies which eigenvalues should be targeted. Valid specifications of <code>which</code> are given by</p><ul><li><code>:LM</code>: eigenvalues of largest magnitude</li><li><code>:LR</code>: eigenvalues with largest (most positive) real part</li><li><code>:SR</code>: eigenvalues with smallest (most negative) real part</li><li><code>:LI</code>: eigenvalues with largest (most positive) imaginary part, only if <code>T &lt;: Complex</code></li><li><code>:SI</code>: eigenvalues with smallest (most negative) imaginary part, only if <code>T &lt;: Complex</code></li><li><a href="#KrylovKit.EigSorter"><code>EigSorter(f; rev = false)</code></a>: eigenvalues <code>λ</code> that appear first (or last if   <code>rev == true</code>) when sorted by <code>f(λ)</code></li></ul><div class="admonition is-info"><header class="admonition-header">Note about selecting `which` eigenvalues</header><div class="admonition-body"><p>Krylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. All of they valid <code>Symbol</code>s for <code>which</code> have this property, but could also be specified using <code>EigSorter</code>, e.g. <code>:LM</code> is equivalent to <code>Eigsorter(abs; rev = true)</code>. Note that smallest magnitude sorting is obtained using e.g. <code>EigSorter(abs; rev = false)</code>, but since no (shift-and)-invert is used, this will only be successful if you somehow know that eigenvalues close to zero are also close to the periphery of the spectrum.</p></div></div><p>The argument <code>T</code> acts as a hint in which <code>Number</code> type the computation should be performed, but is not restrictive. If the linear map automatically produces complex values, complex arithmetic will be used even though <code>T&lt;:Real</code> was specified. However, if the linear map and initial guess are real, approximate eigenvalues will be searched for using a partial Schur factorization, which implies that complex conjugate eigenvalues come in pairs and cannot be split. It is then illegal to choose <code>which</code> in a way that would treat <code>λ</code> and <code>conj(λ)</code> differently, i.e. <code>:LI</code> and <code>:SI</code> are invalid, as well as any <code>EigSorter</code> that would lead to <code>by(λ) != by(conj(λ))</code>.</p><p><strong>Return values:</strong></p><p>The return value is always of the form <code>vals, vecs, info = eigsolve(...)</code> with</p><ul><li><code>vals</code>: a <code>Vector</code> containing the eigenvalues, of length at least <code>howmany</code>, but could   be longer if more eigenvalues were converged at the same cost. Eigenvalues will be real   if <a href="../algorithms/#KrylovKit.Lanczos"><code>Lanczos</code></a> was used and complex if <a href="../algorithms/#KrylovKit.Arnoldi"><code>Arnoldi</code></a> was used (see below).</li><li><code>vecs</code>: a <code>Vector</code> of corresponding eigenvectors, of the same length as <code>vals</code>. Note   that eigenvectors are not returned as a matrix, as the linear map could act on any   custom Julia type with vector like behavior, i.e. the elements of the list <code>vecs</code> are   objects that are typically similar to the starting guess <code>x₀</code>, up to a possibly   different <code>eltype</code>. In particular  for a general matrix (i.e. with <code>Arnoldi</code>) the   eigenvectors are generally complex and are therefore always returned in a complex   number format. When the linear map is a simple <code>AbstractMatrix</code>, <code>vecs</code> will be   <code>Vector{Vector{&lt;:Number}}</code>.</li><li><code>info</code>: an object of type [<code>ConvergenceInfo</code>], which has the following fields<ul><li><code>info.converged::Int</code>: indicates how many eigenvalues and eigenvectors were actually   converged to the specified tolerance <code>tol</code> (see below under keyword arguments)</li><li><code>info.residual::Vector</code>: a list of the same length as <code>vals</code> containing the   residuals <code>info.residual[i] = f(vecs[i]) - vals[i] * vecs[i]</code></li><li><code>info.normres::Vector{&lt;:Real}</code>: list of the same length as <code>vals</code> containing the   norm of the residual <code>info.normres[i] = norm(info.residual[i])</code></li><li><code>info.numops::Int</code>: number of times the linear map was applied, i.e. number of times   <code>f</code> was called, or a vector was multiplied with <code>A</code></li><li><code>info.numiter::Int</code>: number of times the Krylov subspace was restarted (see below)</li></ul></li></ul><div class="admonition is-warning"><header class="admonition-header">Check for convergence</header><div class="admonition-body"><p>No warning is printed if not all requested eigenvalues were converged, so always check if <code>info.converged &gt;= howmany</code>.</p></div></div><p><strong>Keyword arguments:</strong></p><p>Keyword arguments and their default values are given by:</p><ul><li><code>verbosity::Int = 0</code>: verbosity level, i.e. 0 (no messages), 1 (single message   at the end), 2 (information after every iteration), 3 (information per Krylov step)</li><li><code>tol::Real</code>: the requested accuracy (corresponding to the 2-norm of the residual for   Schur vectors, not the eigenvectors). If you work in e.g. single precision (<code>Float32</code>),   you should definitely change the default value.</li><li><code>krylovdim::Integer</code>: the maximum dimension of the Krylov subspace that will be   constructed. Note that the dimension of the vector space is not known or checked, e.g.   <code>x₀</code> should not necessarily support the <code>Base.length</code> function. If you know the actual   problem dimension is smaller than the default value, it is useful to reduce the value of   <code>krylovdim</code>, though in principle this should be detected.</li><li><code>maxiter::Integer</code>: the number of times the Krylov subspace can be rebuilt; see below   for further details on the algorithms.</li><li><code>orth::Orthogonalizer</code>: the orthogonalization method to be used, see   <a href="man/@ref"><code>Orthogonalizer</code></a></li><li><code>issymmetric::Bool</code>: if the linear map is symmetric, only meaningful if <code>T&lt;:Real</code></li><li><code>ishermitian::Bool</code>: if the linear map is hermitian</li><li><code>eager::Bool = false</code>: if true, eagerly compute the eigenvalue or Schur decomposition   after every expansion of the Krylov subspace to test for convergence, otherwise wait   until the Krylov subspace has dimension <code>krylovdim</code></li></ul><p>The default values are given by <code>tol = KrylovDefaults.tol</code>, <code>krylovdim = KrylovDefaults.krylovdim</code>, <code>maxiter = KrylovDefaults.maxiter</code>, <code>orth = KrylovDefaults.orth</code>; see <a href="../algorithms/#KrylovKit.KrylovDefaults"><code>KrylovDefaults</code></a> for details.</p><p>The default value for the last two parameters depends on the method. If an <code>AbstractMatrix</code> is used, <code>issymmetric</code> and <code>ishermitian</code> are checked for that matrix, otherwise the default values are <code>issymmetric = false</code> and <code>ishermitian = T &lt;: Real &amp;&amp; issymmetric</code>. When values for the keyword arguments are provided, no checks will be performed even in the matrix case.</p><p><strong>Algorithm</strong></p><p>The last method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here, one specifies the algorithm explicitly as either <a href="../algorithms/#KrylovKit.Lanczos"><code>Lanczos</code></a>, for real symmetric or complex hermitian problems, or <a href="../algorithms/#KrylovKit.Arnoldi"><code>Arnoldi</code></a>, for general problems. Note that these names refer to the process for building the Krylov subspace, but the actual algorithm is an implementation of the Krylov-Schur algorithm, which can dynamically shrink and grow the Krylov subspace, i.e. the restarts are so-called thick restarts where a part of the current Krylov subspace is kept.</p><div class="admonition is-info"><header class="admonition-header">Note about convergence</header><div class="admonition-body"><p>In case of a general problem, where the <code>Arnoldi</code> method is used, convergence of an eigenvalue is not based on the norm of the residual <code>norm(f(vecs[i]) - vals[i]*vecs[i])</code> for the eigenvector but rather on the norm of the residual for the corresponding Schur vectors.</p><p>See also <a href="#KrylovKit.schursolve"><code>schursolve</code></a> if you want to use the partial Schur decomposition directly, or if you are not interested in computing the eigenvectors, and want to work in real arithmetic all the way true (if the linear map and starting guess are real).</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/KrylovKit.jl/blob/f77b89f22820c8de42638872e612ed71ac481f2d/src/eigsolve/eigsolve.jl#L1-L126">source</a></section></article><p>Which eigenvalues are targeted can be specified using one of the symbols <code>:LM</code>, <code>:LR</code>, <code>:SR</code>, <code>:LI</code> and <code>:SI</code> for largest magnitude, largest and smallest real part, and largest and smallest imaginary part respectively. Alternatively, one can just specify a general sorting operation using <code>EigSorter</code></p><article class="docstring"><header><a class="docstring-binding" id="KrylovKit.EigSorter" href="#KrylovKit.EigSorter"><code>KrylovKit.EigSorter</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EigSorter(by; rev = false)</code></pre><p>A simple <code>struct</code> to be used in combination with <a href="#KrylovKit.eigsolve"><code>eigsolve</code></a> or <a href="#KrylovKit.schursolve"><code>schursolve</code></a> to indicate which eigenvalues need to be targeted, namely those that appear first when sorted by <code>by</code> and possibly in reverse order if <code>rev == true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/KrylovKit.jl/blob/f77b89f22820c8de42638872e612ed71ac481f2d/src/eigsolve/eigsolve.jl#L130-L136">source</a></section></article><p>For a general matrix, eigenvalues and eigenvectors will always be returned with complex values for reasons of type stability. However, if the linear map and initial guess are real, most of the computation is actually performed using real arithmetic, as in fact the first step is to compute an approximate partial Schur factorization. If one is not interested in the eigenvectors, one can also just compute this partial Schur factorization using <code>schursolve</code>.</p><article class="docstring"><header><a class="docstring-binding" id="KrylovKit.schursolve" href="#KrylovKit.schursolve"><code>KrylovKit.schursolve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">schursolve(A, x₀, howmany, which, algorithm)</code></pre><p>Compute a partial Schur decomposition containing <code>howmany</code> eigenvalues from the linear map encoded in the matrix or function <code>A</code>. Return the reduced Schur matrix, the basis of Schur vectors, the extracted eigenvalues and a <code>ConvergenceInfo</code> structure.</p><p>See also <a href="man/@eigsolve"><code>eigsolve</code></a> to obtain the eigenvectors instead. For real symmetric or complex hermitian problems, the (partial) Schur decomposition is identical to the (partial) eigenvalue decomposition, and <code>eigsolve</code> should always be used.</p><p><strong>Arguments:</strong></p><p>The linear map can be an <code>AbstractMatrix</code> (dense or sparse) or a general function or callable object, that acts on vector like objects similar to <code>x₀</code>, which is the starting guess from which a Krylov subspace will be built. <code>howmany</code> specifies how many Schur vectors should be converged before the algorithm terminates; <code>which</code> specifies which eigenvalues should be targeted. Valid specifications of <code>which</code> are</p><ul><li><code>LM</code>: eigenvalues of largest magnitude</li><li><code>LR</code>: eigenvalues with largest (most positive) real part</li><li><code>SR</code>: eigenvalues with smallest (most negative) real part</li><li><code>LI</code>: eigenvalues with largest (most positive) imaginary part, only if <code>T &lt;: Complex</code></li><li><code>SI</code>: eigenvalues with smallest (most negative) imaginary part, only if <code>T &lt;: Complex</code></li><li><a href="man/@ref"><code>ClosestTo(λ)</code></a>: eigenvalues closest to some number <code>λ</code></li></ul><div class="admonition is-info"><header class="admonition-header">Note about selecting `which` eigenvalues</header><div class="admonition-body"><p>Krylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. Even with <code>ClosestTo</code>, no shift and invert is performed. This is useful if, e.g., you know the spectrum to be within the unit circle in the complex plane, and want to target the eigenvalues closest to the value <code>λ = 1</code>.</p></div></div><p>The final argument <code>algorithm</code> can currently only be an instance of <a href="../algorithms/#KrylovKit.Arnoldi"><code>Arnoldi</code></a>, but should nevertheless be specified. Since <code>schursolve</code> is less commonly used as <code>eigsolve</code>, no convenient keyword syntax is currently available.</p><p><strong>Return values:</strong></p><p>The return value is always of the form <code>T, vecs, vals, info = schursolve(...)</code> with</p><ul><li><code>T</code>: a <code>Matrix</code> containing the partial Schur decomposition of the linear map, i.e. it&#39;s   elements are given by <code>T[i,j] = dot(vecs[i], f(vecs[j]))</code>. It is of Schur form, i.e.   upper triangular in case of complex arithmetic, and block upper triangular (with at most   2x2 blocks) in case of real arithmetic.</li><li><code>vecs</code>: a <code>Vector</code> of corresponding Schur vectors, of the same length as <code>vals</code>. Note   that Schur vectors are not returned as a matrix, as the linear map could act on any   custom  Julia type with vector like behavior, i.e. the elements of the list <code>vecs</code> are   objects that are typically similar to the starting guess <code>x₀</code>, up to a possibly   different <code>eltype</code>. When the linear map is a simple <code>AbstractMatrix</code>, <code>vecs</code> will be   <code>Vector{Vector{&lt;:Number}}</code>. Schur vectors are by definition orthogonal, i.e.   <code>dot(vecs[i],vecs[j]) = I[i,j]</code>. Note that Schur vectors are real if the problem (i.e.   the linear map and the initial guess) are real.</li><li><code>vals</code>: a <code>Vector</code> of eigenvalues, i.e. the diagonal elements of <code>T</code> in case of complex   arithmetic, or extracted from the diagonal blocks in case of real arithmetic. Note that   <code>vals</code> will always be complex, independent of the underlying arithmetic.</li><li><code>info</code>: an object of type [<code>ConvergenceInfo</code>], which has the following fields<ul><li><code>info.converged::Int</code>: indicates how many eigenvalues and Schur vectors were   actually converged to the specified tolerance (see below under keyword arguments)</li><li><code>info.residuals::Vector</code>: a list of the same length as <code>vals</code> containing the actual   residuals   <code>julia     info.residuals[i] = f(vecs[i]) - sum(vecs[j]*T[j,i] for j = 1:i+1)</code>   where <code>T[i+1,i]</code> is definitely zero in case of complex arithmetic and possibly zero   in case of real arithmetic</li><li><code>info.normres::Vector{&lt;:Real}</code>: list of the same length as <code>vals</code> containing the   norm of the residual for every Schur vector, i.e.   <code>info.normes[i] = norm(info.residual[i])</code></li><li><code>info.numops::Int</code>: number of times the linear map was applied, i.e. number of times   <code>f</code> was called, or a vector was multiplied with <code>A</code></li><li><code>info.numiter::Int</code>: number of times the Krylov subspace was restarted (see below)</li></ul></li></ul><div class="admonition is-warning"><header class="admonition-header">Check for convergence</header><div class="admonition-body"><p>No warning is printed if not all requested eigenvalues were converged, so always check if <code>info.converged &gt;= howmany</code>.</p></div></div><p><strong>Algorithm</strong></p><p>The actual algorithm is an implementation of the Krylov-Schur algorithm, where the <a href="../algorithms/#KrylovKit.Arnoldi"><code>Arnoldi</code></a> algorithm is used to generate the Krylov subspace. During the algorithm, the Krylov subspace is dynamically grown and shrunk, i.e. the restarts are so-called thick restarts where a part of the current Krylov subspace is kept.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/KrylovKit.jl/blob/f77b89f22820c8de42638872e612ed71ac481f2d/src/eigsolve/arnoldi.jl#L1-L76">source</a></section></article><p>Note that, for symmetric or hermitian linear maps, the eigenvalue and Schur factorization are equivalent, and one can only use <code>eigsolve</code>.</p><p>Another example of a possible use case of <code>schursolve</code> is if the linear map is known to have a unique eigenvalue of, e.g. largest magnitude. Then, if the linear map is real valued, that largest magnitude eigenvalue and its corresponding eigenvector are also real valued. <code>eigsolve</code> will automatically return complex valued eigenvectors for reasons of type stability. However, as the first Schur vector will coincide with the first eigenvector, one can instead use</p><pre><code class="language-julia">T, vecs, vals, info = schursolve(A, x⁠₀, 1, :LM, Arnoldi(...))</code></pre><p>and use <code>vecs[1]</code> as the real valued eigenvector (after checking <code>info.converged</code>) corresponding to the largest magnitude eigenvalue of <code>A</code>.</p><h2 id="Generalized-eigenvalue-problems"><a class="docs-heading-anchor" href="#Generalized-eigenvalue-problems">Generalized eigenvalue problems</a><a id="Generalized-eigenvalue-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-eigenvalue-problems" title="Permalink"></a></h2><p>Generalized eigenvalues <code>λ</code> and corresponding vectors <code>x</code> of the generalized eigenvalue problem <span>$A x = λ B x$</span> can be obtained using the method <code>geneigsolve</code>. Currently, there is only one algorithm, which does not require inverses of <code>A</code> or <code>B</code>, but is restricted to symmetric or hermitian generalized eigenvalue problems where the matrix or linear map <code>B</code> is positive definite. Note that this is not reflected in the default values for the keyword arguments <code>issymmetric</code>, <code>ishermitian</code> and <code>isposdef</code>, so that these should be set explicitly in order to comply with this restriction. If <code>A</code> and <code>B</code> are actual instances of <code>AbstractMatrix</code>, the default value for the keyword arguments will try to check these properties explicitly.</p><article class="docstring"><header><a class="docstring-binding" id="KrylovKit.geneigsolve" href="#KrylovKit.geneigsolve"><code>KrylovKit.geneigsolve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">geneigsolve((A::AbstractMatrix, B::AbstractMatrix), [howmany = 1, which = :LM,
                                T = promote_type(eltype(A), eltype(B))]; kwargs...)
geneigsolve(f, n::Int, [howmany = 1, which = :LM, T = Float64]; kwargs...)
geneigsolve(f, x₀, [howmany = 1, which = :LM]; kwargs...)
geneigsolve(f, x₀, howmany, which, algorithm)</code></pre><p>Compute at least <code>howmany</code> generalized eigenvalues <span>$λ$</span> and generalized eigenvectors <span>$x$</span> of the form <span>$(A - λB)x = 0$</span>, where <code>A</code> and <code>B</code> are either instances of <code>AbstractMatrix</code>, or some function that implements the matrix vector product. In case functions are used, one could either specify the action of <code>A</code> and <code>B</code> via a tuple of two functions (or a function and an <code>AbstractMatrix</code>), or one could use a single function that takes a single argument <code>x</code> and returns two results, corresponding to <code>A*x</code> and <code>B*x</code>. Return the computed eigenvalues, eigenvectors and a <code>ConvergenceInfo</code> structure.</p><p><strong>Arguments:</strong></p><p>The first argument is either a tuple of two linear maps, so a function or an <code>AbstractMatrix</code> for either of them, representing the action of <code>A</code> and <code>B</code>. Alternatively, a single function can be used that takes a single argument <code>x</code> and returns the equivalent of <code>(A*x, B*x)</code> as result. This latter form is compatible with the <code>do</code> block syntax of Julia. If an <code>AbstractMatrix</code> is used for either <code>A</code> or <code>B</code>, a starting vector <code>x₀</code> does not need to be provided, it is then chosen as <code>rand(T, size(A,1))</code> if <code>A</code> is an <code>AbstractMatrix</code> (or similarly if only <code>B</code> is an <code>AbstractMatrix</code>). Here <code>T = promote_type(eltype(A), eltype(B))</code> if both <code>A</code> and <code>B</code> are instances of <code>AbstractMatrix</code>, or just the <code>eltype</code> of whichever is an <code>AbstractMatrix</code>. If both <code>A</code> and <code>B</code> are encoded more generally as a callable function or method, the best approach is to provide an explicit starting guess <code>x₀</code>. Note that <code>x₀</code> does not need to be of type <code>AbstractVector</code>, any type that behaves as a vector and supports the required methods (see KrylovKit docs) is accepted. If instead of <code>x₀</code> an integer <code>n</code> is specified, it is assumed that <code>x₀</code> is a regular vector and it is initialized to <code>rand(T,n)</code>, where the default value of <code>T</code> is <code>Float64</code>, unless specified differently.</p><p>The next arguments are optional, but should typically be specified. <code>howmany</code> specifies how many eigenvalues should be computed; <code>which</code> specifies which eigenvalues should be targeted. Valid specifications of <code>which</code> are given by</p><ul><li><code>:LM</code>: eigenvalues of largest magnitude</li><li><code>:LR</code>: eigenvalues with largest (most positive) real part</li><li><code>:SR</code>: eigenvalues with smallest (most negative) real part</li><li><code>:LI</code>: eigenvalues with largest (most positive) imaginary part, only if <code>T &lt;: Complex</code></li><li><code>:SI</code>: eigenvalues with smallest (most negative) imaginary part, only if <code>T &lt;: Complex</code></li><li><a href="#KrylovKit.EigSorter"><code>EigSorter(f; rev = false)</code></a>: eigenvalues <code>λ</code> that appear first (or last if   <code>rev == true</code>) when sorted by <code>f(λ)</code></li></ul><div class="admonition is-info"><header class="admonition-header">Note about selecting `which` eigenvalues</header><div class="admonition-body"><p>Krylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. Even with <code>ClosestTo</code>, no shift and invert is performed. This is useful if, e.g., you know the spectrum to be within the unit circle in the complex plane, and want to target the eigenvalues closest to the value <code>λ = 1</code>.</p></div></div><p>The argument <code>T</code> acts as a hint in which <code>Number</code> type the computation should be performed, but is not restrictive. If the linear map automatically produces complex values, complex arithmetic will be used even though <code>T&lt;:Real</code> was specified.</p><p><strong>Return values:</strong></p><p>The return value is always of the form <code>vals, vecs, info = geneigsolve(...)</code> with</p><ul><li><code>vals</code>: a <code>Vector</code> containing the eigenvalues, of length at least <code>howmany</code>, but could   be longer if more eigenvalues were converged at the same cost.</li><li><code>vecs</code>: a <code>Vector</code> of corresponding eigenvectors, of the same length as <code>vals</code>.   Note that eigenvectors are not returned as a matrix, as the linear map could act on any   custom Julia type with vector like behavior, i.e. the elements of the list <code>vecs</code> are   objects that are typically similar to the starting guess <code>x₀</code>, up to a possibly   different <code>eltype</code>. When the linear map is a simple <code>AbstractMatrix</code>, <code>vecs</code> will be   <code>Vector{Vector{&lt;:Number}}</code>.</li><li><code>info</code>: an object of type [<code>ConvergenceInfo</code>], which has the following fields<ul><li><code>info.converged::Int</code>: indicates how many eigenvalues and eigenvectors were actually   converged to the specified tolerance <code>tol</code> (see below under keyword arguments)</li><li><code>info.residual::Vector</code>: a list of the same length as <code>vals</code> containing the   residuals <code>info.residual[i] = f(vecs[i]) - vals[i] * vecs[i]</code></li><li><code>info.normres::Vector{&lt;:Real}</code>: list of the same length as <code>vals</code> containing the   norm of the residual <code>info.normres[i] = norm(info.residual[i])</code></li><li><code>info.numops::Int</code>: number of times the linear map was applied, i.e. number of times   <code>f</code> was called, or a vector was multiplied with <code>A</code></li><li><code>info.numiter::Int</code>: number of times the Krylov subspace was restarted (see below)</li></ul></li></ul><div class="admonition is-warning"><header class="admonition-header">Check for convergence</header><div class="admonition-body"><p>No warning is printed if not all requested eigenvalues were converged, so always check if <code>info.converged &gt;= howmany</code>.</p></div></div><p><strong>Keyword arguments:</strong></p><p>Keyword arguments and their default values are given by:</p><ul><li><code>verbosity::Int = 0</code>: verbosity level, i.e. 0 (no messages), 1 (single message   at the end), 2 (information after every iteration), 3 (information per Krylov step)</li><li><code>tol::Real</code>: the requested accuracy (corresponding to the 2-norm of the residual for   Schur vectors, not the eigenvectors). If you work in e.g. single precision (<code>Float32</code>),   you should definitely change the default value.</li><li><code>krylovdim::Integer</code>: the maximum dimension of the Krylov subspace that will be   constructed. Note that the dimension of the vector space is not known or checked, e.g.   <code>x₀</code> should not necessarily support the <code>Base.length</code> function. If you know the actual   problem dimension is smaller than the default value, it is useful to reduce the value   of <code>krylovdim</code>, though in principle this should be detected.</li><li><code>maxiter::Integer</code>: the number of times the Krylov subspace can be rebuilt; see below   for further details on the algorithms.</li><li><code>orth::Orthogonalizer</code>: the orthogonalization method to be used, see   <a href="man/@ref"><code>Orthogonalizer</code></a></li><li><code>issymmetric::Bool</code>: if both linear maps <code>A</code> and <code>B</code> are symmetric, only meaningful if   <code>T&lt;:Real</code></li><li><code>ishermitian::Bool</code>: if both linear maps <code>A</code> and <code>B</code> are hermitian</li><li><code>isposdef::Bool</code>: if the linear map <code>B</code> is positive definite</li></ul><p>The default values are given by <code>tol = KrylovDefaults.tol</code>, <code>krylovdim = KrylovDefaults.krylovdim</code>, <code>maxiter = KrylovDefaults.maxiter</code>, <code>orth = KrylovDefaults.orth</code>; see <a href="../algorithms/#KrylovKit.KrylovDefaults"><code>KrylovDefaults</code></a> for details.</p><p>The default value for the last three parameters depends on the method. If an <code>AbstractMatrix</code> is used, <code>issymmetric</code>, <code>ishermitian</code> and <code>isposdef</code> are checked for that matrix, otherwise the default values are <code>issymmetric = false</code> and <code>ishermitian = T &lt;: Real &amp;&amp; issymmetric</code>. When values are provided, no checks will be performed even in the matrix case.</p><p><strong>Algorithm</strong></p><p>The last method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here the algorithm is specified, though currently only <a href="../algorithms/#KrylovKit.GolubYe"><code>GolubYe</code></a> is available. The Golub-Ye algorithm is an algorithm for solving hermitian (symmetric) generalized eigenvalue problems <code>A x = λ B x</code> with positive definite <code>B</code>, without the need for inverting <code>B</code>. It builds a Krylov subspace of size <code>krylovdim</code> starting from an estimate <code>x</code> by acting with <code>(A - ρ(x) B)</code>, where <code>ρ(x) = dot(x, A*x)/ dot(x, B*x)</code>, and employing the Lanczos algorithm. This process is repeated at most <code>maxiter</code> times. In every iteration <code>k&gt;1</code>, the subspace will also be expanded to size <code>krylovdim+1</code> by adding <span>$x_k - x_{k-1}$</span>, which is known as the LOPCG correction and was suggested by Money and Ye. With <code>krylovdim = 2</code>, this algorithm becomes equivalent to <code>LOPCG</code>.</p><div class="admonition is-warning"><header class="admonition-header">Restriction to symmetric definite generalized eigenvalue problems</header><div class="admonition-body"><p>While the only algorithm so far is restricted to symmetric/hermitian generalized eigenvalue problems with positive definite <code>B</code>, this is not reflected in the default values for the keyword arguments <code>issymmetric</code> or <code>ishermitian</code> and <code>isposdef</code>. Make sure to set these to true to understand the implications of using this algorithm.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/KrylovKit.jl/blob/f77b89f22820c8de42638872e612ed71ac481f2d/src/eigsolve/geneigsolve.jl#L1-L125">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../linear/">« Linear problems</a><a class="docs-footer-nextpage" href="../svd/">Singular value problems »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 14 July 2021 20:18">Wednesday 14 July 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
