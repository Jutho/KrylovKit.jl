var documenterSearchIndex = {"docs":
[{"location":"man/intro/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"Pages = [\"man/intro.md\", \"man/linear.md\", \"man/eig.md\", \"man/svd.md\", \"man/matfun.md\",\n\"man/algorithms.md\", \"man/implementation.md\"]\nDepth = 2","category":"page"},{"location":"man/intro/#Installing","page":"Introduction","title":"Installing","text":"","category":"section"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"Install KrylovKit.jl via the package manager:","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.add(\"KrylovKit\")","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"KrylovKit.jl is a pure Julia package; no dependencies (aside from the Julia standard library) are required.","category":"page"},{"location":"man/intro/#Getting-started","page":"Introduction","title":"Getting started","text":"","category":"section"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"After installation, start by loading KrylovKit","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"using KrylovKit","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"The help entry of the KrylovKit module states","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"KrylovKit","category":"page"},{"location":"man/intro/#KrylovKit","page":"Introduction","title":"KrylovKit","text":"KrylovKit\n\nA Julia package collecting a number of Krylov-based algorithms for linear problems, singular value and eigenvalue problems and the application of functions of linear maps or operators to vectors.\n\nKrylovKit accepts general functions or callable objects as linear maps, and general Julia objects with vector like behavior as vectors.\n\nThe high level interface of KrylovKit is provided by the following functions:\n\nlinsolve: solve linear systems\neigsolve: find a few eigenvalues and corresponding eigenvectors\ngeneigsolve: find a few generalized eigenvalues and corresponding vectors\nsvdsolve: find a few singular values and corresponding left and right singular vectors\nexponentiate: apply the exponential of a linear map to a vector\nexpintegrator: exponential integrator for a linear non-homogeneous ODE, computes a linear combination of the ϕⱼ functions which generalize ϕ₀(z) = exp(z).\n\n\n\n\n\n","category":"module"},{"location":"man/intro/#Common-interface","page":"Introduction","title":"Common interface","text":"","category":"section"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"The for high-level function linsolve, eigsolve, geneigsolve, svdsolve, exponentiate and expintegrator follow a common interface","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"results..., info = problemsolver(A, args...; kwargs...)","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"where problemsolver is one of the functions above. Here, A is the linear map in the problem, which could be an instance of AbstractMatrix, or any function or callable object that encodes the action of the linear map on a vector. In particular, one can write the linear map using Julia's do block syntax as","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"results..., info = problemsolver(args...; kwargs...) do x\n    y = # implement linear map on x\n    return y\nend","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"Read the documentation for problems that require both the linear map and its adjoint to be implemented, e.g. svdsolve, or that require two different linear maps, e.g. geneigsolve.","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"Furthermore, args is a set of additional arguments to specify the problem. The keyword arguments kwargs contain information about the linear map (issymmetric, ishermitian, isposdef) and about the solution strategy (tol, krylovdim, maxiter). Finally, there is a keyword argument verbosity that determines how much information is printed to STDOUT. The default value verbosity = 0 means that no information will be printed. With verbosity = 1, a single message at the end of the algorithm will be displayed, which is a warning if the algorithm did not succeed in finding the solution, or some information if it did. For verbosity = 2, information about the current state is displayed after every iteration of the algorithm. Finally, for verbosity > 2, information about the individual Krylov expansion steps is displayed.","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"The return value contains one or more entries that define the solution, and a final entry info of type ConvergeInfo that encodes information about the solution, i.e. whether it has converged, the residual(s) and the norm thereof, the number of operations used:","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"KrylovKit.ConvergenceInfo","category":"page"},{"location":"man/intro/#KrylovKit.ConvergenceInfo","page":"Introduction","title":"KrylovKit.ConvergenceInfo","text":"struct ConvergenceInfo{S,T}\n    converged::Int\n    residual::T\n    normres::S\n    numiter::Int\n    numops::Int\nend\n\nUsed to return information about the solution found by the iterative method.\n\nconverged: the number of solutions that have converged according to an appropriate error measure and requested tolerance for the problem. Its value can be zero or one for linsolve, exponentiate and  expintegrator, or any integer >= 0 for eigsolve, schursolve or svdsolve.\nresidual: the (list of) residual(s) for the problem, or nothing for problems without the concept of a residual (i.e. exponentiate, expintegrator). This is a single vector (of the same type as the type of vectors used in the problem) for linsolve, or a Vector of such vectors for eigsolve, schursolve or svdsolve.\nnormres: the norm of the residual(s) (in the previous field) or the value of any other error measure that is appropriate for the problem. This is a Real for linsolve and exponentiate, and a Vector{<:Real} for eigsolve, schursolve and svdsolve. The number of values in normres that are smaller than a predefined tolerance corresponds to the number converged of solutions that have converged.\nnumiter: the number of iterations (sometimes called restarts) used by the algorithm.\nnumops: the number of times the linear map or operator was applied\n\n\n\n\n\n","category":"type"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"There is also an expert interface where the user specifies the algorithm that should be used explicitly, i.e.","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"results..., info = problemsolver(A, args..., algorithm(; kwargs...))","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"Most algorithm constructions take the same keyword arguments (tol, krylovdim, maxiter and verbosity) discussed above.","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"While KrylovKit.jl does currently not provide a general interface for including preconditioners, it is possible to e.g. use a modified inner product. KrylovKit.jl provides a specific type for this purpose:","category":"page"},{"location":"man/intro/","page":"Introduction","title":"Introduction","text":"InnerProductVec","category":"page"},{"location":"man/intro/#KrylovKit.InnerProductVec","page":"Introduction","title":"KrylovKit.InnerProductVec","text":"v = InnerProductVec(vec, dotf)\n\nCreate a new vector v from an existing vector dotf with a modified inner product given by inner. The vector vec, which can be any type (not necessarily Vector) that supports the basic vector interface required by KrylovKit, is wrapped in a custom struct v::InnerProductVec. All vector space functionality such as addition and multiplication with scalars (both out of place and in place using mul!, rmul!, axpy! and axpby!) applied to v is simply forwarded to v.vec. The inner product between vectors v1 = InnerProductVec(vec1, dotf) and v2 = InnerProductVec(vec2, dotf) is computed as dot(v1, v2) = dotf(v1.vec, v2.vec) = dotf(vec1, vec2). The inner product between vectors with different dotf functions is not defined. Similarly, The norm of v::InnerProductVec is defined as v = sqrt(real(dot(v, v))) = sqrt(real(dotf(vec, vec))).\n\nIn a (linear) map applied to v, the original vector can be obtained as v.vec or simply as v[].\n\n\n\n\n\n","category":"type"},{"location":"man/matfun/#Functions-of-matrices-and-linear-operators","page":"Functions of matrices and linear operators","title":"Functions of matrices and linear operators","text":"","category":"section"},{"location":"man/matfun/","page":"Functions of matrices and linear operators","title":"Functions of matrices and linear operators","text":"Applying a function of a matrix or linear operator to a given vector can in some cases also be computed using Krylov methods. One example is the inverse function, which exactly corresponds to what linsolve computes: A^-1 * b. There are other functions f for which f(A) * b can be computed using Krylov techniques, i.e. where f(A) * b can be well approximated in the Krylov subspace spanned by b A * b A^2 * b .","category":"page"},{"location":"man/matfun/","page":"Functions of matrices and linear operators","title":"Functions of matrices and linear operators","text":"Currently, the only family of functions of a linear map for which such a method is available are the ϕⱼ(z) functions which generalize the exponential function ϕ₀(z) = exp(z) and arise in the context of linear non-homogeneous ODEs. The corresponding Krylov method for computing is an exponential integrator, and is thus available under the name expintegrator. For a linear homogeneous ODE, the solution is a pure exponential, and the special wrapper exponentiate is available:","category":"page"},{"location":"man/matfun/","page":"Functions of matrices and linear operators","title":"Functions of matrices and linear operators","text":"exponentiate\nexpintegrator","category":"page"},{"location":"man/matfun/#KrylovKit.exponentiate","page":"Functions of matrices and linear operators","title":"KrylovKit.exponentiate","text":"function exponentiate(A, t::Number, x; kwargs...)\nfunction exponentiate(A, t::Number, x, algorithm)\n\nCompute y = exp(t*A) x, where A is a general linear map, i.e. a AbstractMatrix or just a general function or callable object and x is of any Julia type with vector like behavior.\n\nArguments:\n\nThe linear map A can be an AbstractMatrix (dense or sparse) or a general function or callable object that implements the action of the linear map on a vector. If A is an AbstractMatrix, x is expected to be an AbstractVector, otherwise x can be of any type that behaves as a vector and supports the required methods (see KrylovKit docs).\n\nThe time parameter t can be real or complex, and it is better to choose t e.g. imaginary and A hermitian, then to absorb the imaginary unit in an antihermitian A. For the former, the Lanczos scheme is used to built a Krylov subspace, in which an approximation to the exponential action of the linear map is obtained. The argument x can be of any type and should be in the domain of A.\n\nReturn values:\n\nThe return value is always of the form y, info = exponentiate(...) with\n\ny: the result of the computation, i.e. y = exp(t*A)*x\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: 0 or 1 if the solution y at time t was found with an error below the requested tolerance per unit time, i.e. if info.normres <= tol * abs(t)\ninfo.residual::Nothing: value nothing, there is no concept of a residual in this case\ninfo.normres::Real: a (rough) estimate of the total error accumulated in the solution\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nBy default (i.e. if verbosity = 0, see below), no warning is printed if the solution was not found with the requested precision, so be sure to check info.converged == 1.\n\nKeyword arguments:\n\nKeyword arguments and their default values are given by:\n\nverbosity::Int = 0: verbosity level, i.e. \n0 (suppress all messages)\n1 (only warnings)\n2 (one message with convergence info at the end)\n3 (progress info after every iteration)\n4+ (all of the above and additional information about the Lanczos or Arnoldi iteration)\nkrylovdim = 30: the maximum dimension of the Krylov subspace that will be constructed. Note that the dimension of the vector space is not known or checked, e.g. x₀ should not necessarily support the Base.length function. If you know the actual problem dimension is smaller than the default value, it is useful to reduce the value of krylovdim, though in principle this should be detected.\ntol = 1e-12: the requested accuracy per unit time, i.e. if you want a certain precision ϵ on the final result, set tol = ϵ/abs(t). If you work in e.g. single precision (Float32), you should definitely change the default value.\nmaxiter::Int = 100: the number of times the Krylov subspace can be rebuilt; see below for further details on the algorithms.\nissymmetric: if the linear map is symmetric, only meaningful if T<:Real\nishermitian: if the linear map is hermitian The default value for the last two depends on the method. If an AbstractMatrix is used, issymmetric and ishermitian are checked for that matrix, otherwise the default values are issymmetric = false and ishermitian = T <: Real && issymmetric.\neager::Bool = false: if true, eagerly try to compute the result after every expansion of the Krylov subspace to test for convergence, otherwise wait until the Krylov subspace as dimension krylovdim. This can result in a faster return, for example if the total time for the evolution is quite small, but also has some overhead, as more computations are performed after every expansion step.\n\nAlgorithm\n\nThis is actually a simple wrapper over more general method expintegrator for for integrating a linear non-homogeneous ODE.\n\n\n\n\n\n","category":"function"},{"location":"man/matfun/#KrylovKit.expintegrator","page":"Functions of matrices and linear operators","title":"KrylovKit.expintegrator","text":"function expintegrator(A, t::Number, u₀, u₁, …; kwargs...)\nfunction expintegrator(A, t::Number, (u₀, u₁, …); kwargs...)\nfunction expintegrator(A, t::Number, (u₀, u₁, …), algorithm)\n\nCompute y = ϕ₀(t*A)*u₀ + t*ϕ₁(t*A)*u₁ + t^2*ϕ₂(t*A)*u₂ + , where A is a general linear map, i.e. a AbstractMatrix or just a general function or callable object and u₀, u₁ are of any Julia type with vector like behavior. Here, ϕ₀(z) = exp(z) and ϕⱼ₁ = (ϕⱼ(z) - 1j)z. In particular, y = x(t) represents the solution of the ODE x(t) = A*x(t) + ⱼ t^jj uⱼ₁ with x(0) = u₀.\n\nnote: Note\nWhen there are only input vectors u₀ and u₁, t can equal Inf, in which the algorithm tries to evolve all the way to the fixed point y = - A \\ u₁ + P₀ u₀ with P₀ the projector onto the eigenspace of eigenvalue zero (if any) of A. If A has any eigenvalues with real part larger than zero, however, the solution to the ODE will diverge, i.e. the fixed point is not stable.\n\nArguments:\n\nThe linear map A can be an AbstractMatrix (dense or sparse) or a general function or callable object that implements the action of the linear map on a vector. If A is an AbstractMatrix, x is expected to be an AbstractVector, otherwise x can be of any type that behaves as a vector and supports the required methods (see KrylovKit docs).\n\nThe time parameter t can be real or complex, and it is better to choose t e.g. imaginary and A hermitian, then to absorb the imaginary unit in an antihermitian A. For the former, the Lanczos scheme is used to built a Krylov subspace, in which an approximation to the exponential action of the linear map is obtained. The arguments u₀, u₁, … can be of any type and should be in the domain of A.\n\nReturn values:\n\nThe return value is always of the form y, info = expintegrator(...) with\n\ny: the result of the computation, i.e. y = ϕ₀(t*A)*u₀ + t*ϕ₁(t*A)*u₁ + t^2*ϕ₂(t*A)*u₂ + \ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: 0 or 1 if the solution y at time t was found with an error below the requested tolerance per unit time, i.e. if info.normres <= tol * abs(t)\ninfo.residual::Nothing: value nothing, there is no concept of a residual in this case\ninfo.normres::Real: a (rough) estimate of the total error accumulated in the solution\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nKeyword arguments:\n\nKeyword arguments and their default values are given by:\n\nverbosity::Int = 0: verbosity level, i.e. \n0 (suppress all messages)\n1 (only warnings)\n2 (one message with convergence info at the end)\n3 (progress info after every iteration)\n4+ (all of the above and additional information about the Lanczos or Arnoldi iteration)\nkrylovdim = 30: the maximum dimension of the Krylov subspace that will be constructed. Note that the dimension of the vector space is not known or checked, e.g. x₀ should not necessarily support the Base.length function. If you know the actual problem dimension is smaller than the default value, it is useful to reduce the value of krylovdim, though in principle this should be detected.\ntol = 1e-12: the requested accuracy per unit time, i.e. if you want a certain precision ϵ on the final result, set tol = ϵ/abs(t). If you work in e.g. single precision (Float32), you should definitely change the default value.\nmaxiter::Int = 100: the number of times the Krylov subspace can be rebuilt; see below for further details on the algorithms.\nissymmetric: if the linear map is symmetric, only meaningful if T<:Real\nishermitian: if the linear map is hermitian The default value for the last two depends on the method. If an AbstractMatrix is used, issymmetric and ishermitian are checked for that matrix, otherwise the default values are issymmetric = false and ishermitian = T <: Real && issymmetric.\neager::Bool = false: if true, eagerly try to compute the result after every expansion of the Krylov subspace to test for convergence, otherwise wait until the Krylov subspace as dimension krylovdim. This can result in a faster return, for example if the total time for the evolution is quite small, but also has some overhead, as more computations are performed after every expansion step.\n\nAlgorithm\n\nThe last method, without keyword arguments and the different vectors u₀, u₁, … in a tuple, is the one that is finally called, and can also be used directly. Here, one specifies the algorithm explicitly as either Lanczos, for real symmetric or complex hermitian linear maps, or Arnoldi, for general linear maps. Note that these names refer to the process for building the Krylov subspace, and that one can still use complex time steps in combination with e.g. a real symmetric map.\n\n\n\n\n\n","category":"function"},{"location":"man/algorithms/#Available-algorithms","page":"Available algorithms","title":"Available algorithms","text":"","category":"section"},{"location":"man/algorithms/#Orthogonalization-algorithms","page":"Available algorithms","title":"Orthogonalization algorithms","text":"","category":"section"},{"location":"man/algorithms/","page":"Available algorithms","title":"Available algorithms","text":"KrylovKit.Orthogonalizer\nClassicalGramSchmidt\nModifiedGramSchmidt\nClassicalGramSchmidt2\nModifiedGramSchmidt2\nClassicalGramSchmidtIR\nModifiedGramSchmidtIR","category":"page"},{"location":"man/algorithms/#KrylovKit.Orthogonalizer","page":"Available algorithms","title":"KrylovKit.Orthogonalizer","text":"abstract type Orthogonalizer\n\nSupertype of a hierarchy for representing different orthogonalization strategies or algorithms.\n\nSee also: ClassicalGramSchmidt, ModifiedGramSchmidt, ClassicalGramSchmidt2, ModifiedGramSchmidt2, ClassicalGramSchmidtIR, ModifiedGramSchmidtIR.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.ClassicalGramSchmidt","page":"Available algorithms","title":"KrylovKit.ClassicalGramSchmidt","text":"ClassicalGramSchmidt()\n\nRepresents the classical Gram Schmidt algorithm for orthogonalizing different vectors, typically not an optimal choice.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.ModifiedGramSchmidt","page":"Available algorithms","title":"KrylovKit.ModifiedGramSchmidt","text":"ModifiedGramSchmidt()\n\nRepresents the modified Gram Schmidt algorithm for orthogonalizing different vectors, typically a reasonable choice for linear systems but not for eigenvalue solvers with a large Krylov dimension.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.ClassicalGramSchmidt2","page":"Available algorithms","title":"KrylovKit.ClassicalGramSchmidt2","text":"ClassicalGramSchmidt2()\n\nRepresents the classical Gram Schmidt algorithm with a second reorthogonalization step always taking place.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.ModifiedGramSchmidt2","page":"Available algorithms","title":"KrylovKit.ModifiedGramSchmidt2","text":"ModifiedGramSchmidt2()\n\nRepresents the modified Gram Schmidt algorithm with a second reorthogonalization step always taking place.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.ClassicalGramSchmidtIR","page":"Available algorithms","title":"KrylovKit.ClassicalGramSchmidtIR","text":"ClassicalGramSchmidtIR(η::Real = 1/sqrt(2))\n\nRepresents the classical Gram Schmidt algorithm with iterative (i.e. zero or more) reorthogonalization until the norm of the vector after an orthogonalization step has not decreased by a factor smaller than η with respect to the norm before the step. The default value corresponds to the Daniel-Gragg-Kaufman-Stewart condition.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.ModifiedGramSchmidtIR","page":"Available algorithms","title":"KrylovKit.ModifiedGramSchmidtIR","text":"ModifiedGramSchmidtIR(η::Real = 1/sqrt(2))\n\nRepresents the modified Gram Schmidt algorithm with iterative (i.e. zero or more) reorthogonalization until the norm of the vector after an orthogonalization step has not decreased by a factor smaller than η with respect to the norm before the step. The default value corresponds to the Daniel-Gragg-Kaufman-Stewart condition.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#General-Krylov-algorithms","page":"Available algorithms","title":"General Krylov algorithms","text":"","category":"section"},{"location":"man/algorithms/","page":"Available algorithms","title":"Available algorithms","text":"Lanczos\nArnoldi","category":"page"},{"location":"man/algorithms/#KrylovKit.Lanczos","page":"Available algorithms","title":"KrylovKit.Lanczos","text":"Lanczos(; krylovdim=KrylovDefaults.krylovdim[],\n        maxiter=KrylovDefaults.maxiter[],\n        tol=KrylovDefaults.tol[],\n        orth=KrylovDefaults.orth,\n        eager=false,\n        verbosity=KrylovDefaults.verbosity[])\n\nRepresents the Lanczos algorithm for building the Krylov subspace; assumes the linear operator is real symmetric or complex Hermitian. Can be used in eigsolve and exponentiate. The corresponding algorithms will build a Krylov subspace of size at most krylovdim, which will be repeated at most maxiter times and will stop when the norm of the residual of the Lanczos factorization is smaller than tol. The orthogonalizer orth will be used to orthogonalize the different Krylov vectors. Eager mode, as selected by eager=true, means that the algorithm that uses this Lanczos process (e.g. eigsolve) can try to finish its computation before the total Krylov subspace of dimension krylovdim is constructed. The default verbosity level verbosity amounts to printing warnings upon lack of convergence.\n\nUse Arnoldi for non-symmetric or non-Hermitian linear operators.\n\nSee also: factorize, eigsolve, exponentiate, Arnoldi, Orthogonalizer\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.Arnoldi","page":"Available algorithms","title":"KrylovKit.Arnoldi","text":"Arnoldi(; krylovdim=KrylovDefaults.krylovdim[],\n        maxiter=KrylovDefaults.maxiter[],\n        tol=KrylovDefaults.tol[],\n        orth=KrylovDefaults.orth,\n        eager=false,\n        verbosity=KrylovDefaults.verbosity[])\n\nRepresents the Arnoldi algorithm for building the Krylov subspace for a general matrix or linear operator. Can be used in eigsolve and exponentiate. The corresponding algorithms will build a Krylov subspace of size at most krylovdim, which will be repeated at most maxiter times and will stop when the norm of the residual of the Arnoldi factorization is smaller than tol. The orthogonalizer orth will be used to orthogonalize the different Krylov vectors. Eager mode, as selected by eager=true, means that the algorithm that uses this Arnoldi process (e.g. eigsolve) can try to finish its computation before the total Krylov subspace of dimension krylovdim is constructed. The default verbosity level verbosity amounts to printing warnings upon lack of convergence.\n\nUse Lanczos for real symmetric or complex Hermitian linear operators.\n\nSee also: eigsolve, exponentiate, Lanczos, Orthogonalizer\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#Specific-algorithms-for-linear-systems","page":"Available algorithms","title":"Specific algorithms for linear systems","text":"","category":"section"},{"location":"man/algorithms/","page":"Available algorithms","title":"Available algorithms","text":"CG\nKrylovKit.MINRES\nGMRES\nKrylovKit.BiCG\nBiCGStab\nLSMR","category":"page"},{"location":"man/algorithms/#KrylovKit.CG","page":"Available algorithms","title":"KrylovKit.CG","text":"CG(; maxiter=KrylovDefaults.maxiter[], tol=KrylovDefaults.tol[], verbosity=KrylovDefaults.verbosity[])\n\nConstruct an instance of the conjugate gradient algorithm with specified parameters, which can be passed to linsolve in order to iteratively solve a linear system with a positive definite (and thus symmetric or hermitian) coefficient matrix or operator. The CG method will search for the optimal x in a Krylov subspace of maximal size maxiter, or stop when norm(A*x - b) < tol. The default verbosity level verbosity amounts to printing warnings upon lack of convergence.\n\nSee also: linsolve, MINRES, GMRES, BiCG, LSMR, BiCGStab\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.MINRES","page":"Available algorithms","title":"KrylovKit.MINRES","text":"MINRES(; maxiter=KrylovDefaults.maxiter[], tol=KrylovDefaults.tol[], verbosity=KrylovDefaults.verbosity[])\n\n!!! warning \"Not implemented yet\"\n\nConstruct an instance of the conjugate gradient algorithm with specified parameters,\nwhich can be passed to `linsolve` in order to iteratively solve a linear system with a\nreal symmetric or complex hermitian coefficient matrix or operator. The `MINRES` method\nwill search for the optimal `x` in a Krylov subspace of maximal size `maxiter`, or stop\nwhen `norm(A*x - b) < tol`. In building the Krylov subspace, `MINRES` will use the\northogonalizer `orth`. The default verbosity level `verbosity` amounts to printing\nwarnings upon lack of convergence.\n\nSee also: linsolve, CG, GMRES, BiCG, LSMR, BiCGStab\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.GMRES","page":"Available algorithms","title":"KrylovKit.GMRES","text":"GMRES(; krylovdim=KrylovDefaults.krylovdim[],\n        maxiter=KrylovDefaults.maxiter[],\n        tol=KrylovDefaults.tol[], \n        orth::Orthogonalizer=KrylovDefaults.orth,\n        verbosity=KrylovDefaults.verbosity[])\n\nConstruct an instance of the GMRES algorithm with specified parameters, which can be passed to linsolve in order to iteratively solve a linear system. The GMRES method will search for the optimal x in a Krylov subspace of maximal size krylovdim, and repeat this process for at most maxiter times, or stop when norm(A*x - b) < tol. In building the Krylov subspace, GMRES will use the orthogonalizer orth. The default verbosity level verbosity amounts to printing warnings upon lack of convergence.\n\nNote that in the traditional nomenclature of GMRES, the parameter krylovdim is referred to as the restart parameter, and maxiter is the number of outer iterations, i.e. restart cycles. The total iteration count, i.e. the number of expansion steps, is roughly krylovdim times the number of iterations.\n\nSee also: linsolve, BiCG, BiCGStab, CG, LSMR, MINRES\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.BiCG","page":"Available algorithms","title":"KrylovKit.BiCG","text":"BiCG(; maxiter=KrylovDefaults.maxiter[], tol=KrylovDefaults.tol[], verbosity=KrylovDefaults.verbosity[])\n\n!!! warning \"Not implemented yet\"\n\nConstruct an instance of the Biconjugate gradient algorithm with specified parameters,\nwhich can be passed to `linsolve` in order to iteratively solve a linear system general\nlinear map, of which the adjoint can also be applied. The `BiCG` method will search for\nthe optimal `x` in a Krylov subspace of maximal size `maxiter`, or stop when `norm(A*x -\nb) < tol`. The default verbosity level `verbosity` amounts to printing warnings upon\nlack of convergence.\n\nSee also: linsolve, GMRES, CG, BiCGStab, LSMR, MINRES\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.BiCGStab","page":"Available algorithms","title":"KrylovKit.BiCGStab","text":"BiCGStab(; maxiter=KrylovDefaults.maxiter[], tol=KrylovDefaults.tol[], verbosity=KrylovDefaults.verbosity[])\n\nConstruct an instance of the Biconjugate gradient algorithm with specified parameters,\nwhich can be passed to `linsolve` in order to iteratively solve a linear system general\nlinear map. The `BiCGStab` method will search for the optimal `x` in a Krylov subspace\nof maximal size `maxiter`, or stop when `norm(A*x - b) < tol`. The default verbosity level \n`verbosity` amounts to printing warnings upon lack of convergence.\n\nSee also: linsolve, GMRES, CG, BiCG, LSMR, MINRES\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#KrylovKit.LSMR","page":"Available algorithms","title":"KrylovKit.LSMR","text":"LSMR(; krylovdim=1,\n        maxiter=KrylovDefaults.maxiter[],\n        tol=KrylovDefaults.tol[], \n        orth::Orthogonalizer=ModifiedGramSchmidt(),\n        verbosity=KrylovDefaults.verbosity[])\n\nRepresents the LSMR algorithm, which minimizes Ax - b^2 + λx^2 in the Euclidean norm. If multiple solutions exists the minimum norm solution is returned. The method is based on the Golub-Kahan bidiagonalization process. It is algebraically equivalent to applying MINRES to the normal equations (A^*A + λ^2I)x = A^*b, but has better numerical properties, especially if A is ill-conditioned.\n\nThe LSMR method will search for the optimal x in a Krylov subspace of maximal size  maxiter, or stop when norm(A*(A*x - b) + λ^2 * x)  tol. The parameter krylovdim does in this case not indicate that a subspace of that size will be built, but represents the number of most recent vectors that will be kept to which the next vector will be reorthogonalized. The default verbosity level verbosity amounts to printing warnings upon lack of convergence.\n\nSee also: lssolve\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#Specific-algorithms-for-generalized-eigenvalue-problems","page":"Available algorithms","title":"Specific algorithms for generalized eigenvalue problems","text":"","category":"section"},{"location":"man/algorithms/","page":"Available algorithms","title":"Available algorithms","text":"GolubYe","category":"page"},{"location":"man/algorithms/#KrylovKit.GolubYe","page":"Available algorithms","title":"KrylovKit.GolubYe","text":"GolubYe(; krylovdim=KrylovDefaults.krylovdim[],\n        maxiter=KrylovDefaults.maxiter[],\n        tol=KrylovDefaults.tol[],\n        orth=KrylovDefaults.orth,\n        eager=false,\n        verbosity=KrylovDefaults.verbosity[])\n\nRepresents the Golub-Ye algorithm for solving hermitian (symmetric) generalized eigenvalue problems A x = λ B x with positive definite B, without the need for inverting B. Builds a Krylov subspace of size krylovdim starting from an estimate x by acting with (A - ρ(x) B), where ρ(x) = dot(x, A*x)/dot(x, B*x), and employing the Lanczos algorithm. This process is repeated at most maxiter times. In every iteration k>1, the subspace will also be expanded to size krylovdim+1 by adding x_k - x_k-1, which is known as the LOPCG correction and was suggested by Money and Ye. With krylovdim=2, this algorithm becomes equivalent to LOPCG.\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#Specific-algorithms-for-singular-value-problems","page":"Available algorithms","title":"Specific algorithms for singular value problems","text":"","category":"section"},{"location":"man/algorithms/","page":"Available algorithms","title":"Available algorithms","text":"GKL","category":"page"},{"location":"man/algorithms/#KrylovKit.GKL","page":"Available algorithms","title":"KrylovKit.GKL","text":"GKL(; krylovdim=KrylovDefaults.krylovdim[],\n    maxiter=KrylovDefaults.maxiter[],\n    tol=KrylovDefaults.tol[],\n    orth=KrylovDefaults.orth,\n    eager=false,\n    verbosity=KrylovDefaults.verbosity[])\n\nRepresents the Golub-Kahan-Lanczos bidiagonalization algorithm for sequentially building a Krylov-like factorization of a general matrix or linear operator with a bidiagonal reduced matrix. Can be used in svdsolve. The corresponding algorithm builds a Krylov subspace of size at most krylovdim, which will be repeated at most maxiter times and will stop when the norm of the residual of the Arnoldi factorization is smaller than tol. The orthogonalizer orth will be used to orthogonalize the different Krylov vectors. The default verbosity level verbosity amounts to printing warnings upon lack of convergence.\n\nSee also: svdsolve, Orthogonalizer\n\n\n\n\n\n","category":"type"},{"location":"man/algorithms/#Default-values","page":"Available algorithms","title":"Default values","text":"","category":"section"},{"location":"man/algorithms/","page":"Available algorithms","title":"Available algorithms","text":"KrylovDefaults","category":"page"},{"location":"man/algorithms/#KrylovKit.KrylovDefaults","page":"Available algorithms","title":"KrylovKit.KrylovDefaults","text":"module KrylovDefaults\n    const orth = KrylovKit.ModifiedGramSchmidtIR()\n    const krylovdim = Ref(30)\n    const maxiter = Ref(100)\n    const tol = Ref(1e-12)\n    const verbosity = Ref(KrylovKit.WARN_LEVEL)\nend\n\nA module listing the default values for the typical parameters in Krylov based algorithms:\n\north = ModifiedGramSchmidtIR(): the orthogonalization routine used to orthogonalize the Krylov basis in the Lanczos or Arnoldi iteration\nkrylovdim = 30: the maximal dimension of the Krylov subspace that will be constructed\nmaxiter = 100: the maximal number of outer iterations, i.e. the maximum number of times the Krylov subspace may be rebuilt\ntol = 1e-12: the tolerance to which the problem must be solved, based on a suitable error measure, e.g. the norm of some residual.\n\nwarning: Warning\nThe default value of tol is a Float64 value, if you solve problems in Float32 or ComplexF32 arithmetic, you should always specify a new tol as the default value will not be attainable.\n\n\n\n\n\n","category":"module"},{"location":"man/leastsquares/#Least-squares-problems","page":"Least squares problems","title":"Least squares problems","text":"","category":"section"},{"location":"man/leastsquares/","page":"Least squares problems","title":"Least squares problems","text":"Least square problems take the form of finding x that minimises norm(b - A*x) where A should be a linear map. As opposed to linear systems, the input and output of the linear map do not need to be the same, so that x (input) and b (output) can live in different vector spaces. Such problems can be solved using the function lssolve:","category":"page"},{"location":"man/leastsquares/","page":"Least squares problems","title":"Least squares problems","text":"lssolve","category":"page"},{"location":"man/leastsquares/#KrylovKit.lssolve","page":"Least squares problems","title":"KrylovKit.lssolve","text":"lssolve(A::AbstractMatrix, b::AbstractVector, [λ = 0]; kwargs...)\nlssolve(f, b, [λ = 0]; kwargs...)\n# expert version:\nlssolve(f, b, algorithm, [λ = 0])\n\nCompute a least squares solution x to the problem A * x ≈ b or f(x) ≈ b where f encodes a linear map, i.e. a solution x that minimizes norm(b - f(x)). Return the approximate solution x and a ConvergenceInfo structure.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object. Since both the action of the linear map and its adjoint are required in order to solve the least squares problem, f can either be a tuple of two callable objects (each accepting a single argument), representing the linear map and its adjoint respectively,  or, f can be a single callable object that accepts two input arguments, where the second argument is a flag of type Val{true} or Val{false} that indicates whether the adjoint or the normal action of the linear map needs to be computed. The latter form still combines well with the do block syntax of Julia, as in\n\nx, info = lssolve(b; kwargs...) do x, flag\n    if flag === Val(true)\n        # y = compute action of adjoint map on x\n    else\n        # y = compute action of linear map on x\n    end\n    return y\nend\n\nIf the linear map A or f has a nontrivial nullspace, so different minimisers exist, the solution being returned is such that norm(x) is minimal. Alternatively, the problem can be providing a nonzero value for the optional argument λ, representing a scalar so  that the minimisation problem norm(b - A * x)^2 + λ * norm(x)^2 is solved instead.\n\ninfo: Starting guess\nNote that lssolve does not allow to specify an starting guess x₀ for the solution. The starting guess is always assumed to be the zero vector in the domain of the linear map, which is found by applying the adjoint action of the linear map to b and applying zerovector to the result. Given a good initial guess x₀, the user can call lssolve with a modified right hand side b - f(x₀) and add x₀ to the solution returned by lssolve. The resulting vector x is a least squares solution to the original problem, but such that  norm(x - x₀) is minimal or norm(b - A * x)^2 + λ * norm(x-x₀)^2 is minimised instead\n\nReturn values:\n\nThe return value is always of the form x, info = lssolve(...) with\n\nx: the least squares solution to the problem, as defined above \ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: takes value 0 or 1 depending on whether the solution was converged up to the requested tolerance\ninfo.residual: residual b - A*x of the approximate solution x\ninfo.normres::Real: norm of the residual of the normal equations, i.e. the quantity norm(A'*(b - A*x) - λ^2 * x) that needs to be smaller than the requested tolerance tol in order to have a converged solution\ninfo.numops::Int: total number of times that the linear map was applied, i.e. the number of times that f was called, or a vector was multiplied with A or A'\ninfo.numiter::Int: total number of iterations of the algorithm\n\nwarning: Check for convergence\nNo warning is printed if no converged solution was found, so always check if info.converged == 1.\n\nKeyword arguments:\n\nKeyword arguments are given by:\n\nverbosity::Int = 0: verbosity level, i.e. \n0 (suppress all messages)\n1 (only warnings)\n2 (information at the beginning and end)\n3 (progress info after every iteration)\natol::Real: the requested accuracy, i.e. absolute tolerance, on the norm of the residual.\nrtol::Real: the requested accuracy on the norm of the residual, relative to the norm of the right hand side b.\ntol::Real: the requested accuracy on the norm of the residual that is actually used by the algorithm; it defaults to max(atol, rtol*norm(b)). So either use atol and rtol or directly use tol (in which case the value of atol and rtol will be ignored).\nmaxiter::Integer: the number of iterations of the algorithm. Every iteration involves one application of the linear map and one application of the adjoint of the linear map.\n\nThe default values are given by atol = KrylovDefaults.tol, rtol = KrylovDefaults.tol, tol = max(atol, rtol*norm(b)), maxiter = KrylovDefaults.maxiter; see KrylovDefaults for details.\n\nAlgorithms\n\nThe final (expert) method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here, one specifies the algorithm explicitly. Currently, only LSMR is available and thus selected.\n\n\n\n\n\n","category":"function"},{"location":"man/svd/#Singular-value-problems","page":"Singular value problems","title":"Singular value problems","text":"","category":"section"},{"location":"man/svd/#Singular-values-and-singular-vectors","page":"Singular value problems","title":"Singular values and singular vectors","text":"","category":"section"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"It is possible to iteratively compute a few singular values and corresponding left and right singular vectors using the function svdsolve:","category":"page"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"svdsolve","category":"page"},{"location":"man/svd/#KrylovKit.svdsolve","page":"Singular value problems","title":"KrylovKit.svdsolve","text":"svdsolve(A::AbstractMatrix, [x₀, howmany = 1, which = :LR, T = eltype(A)]; kwargs...)\nsvdsolve(f, m::Int, [howmany = 1, which = :LR, T = Float64]; kwargs...)\nsvdsolve(f, x₀, [howmany = 1, which = :LR]; kwargs...)\n# expert version:\nsvdsolve(f, x₀, howmany, which, algorithm; alg_rrule=...)\n\nCompute howmany singular values from the linear map encoded in the matrix A or by the function f. Return singular values, left and right singular vectors and a ConvergenceInfo structure.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object. Since both the action of the linear map and its adjoint are required in order to compute singular values, f can either be a tuple of two callable objects (each accepting a single argument), representing the linear map and its adjoint respectively, or, f can be a single callable object that accepts two input arguments, where the second argument is a flag of type Val{true} or Val{false} that indicates whether the adjoint or the normal action of the linear map needs to be computed. The latter form still combines well with the do block syntax of Julia, as in\n\nvals, lvecs, rvecs, info = svdsolve(x₀, howmany, which; kwargs...) do x, flag\n    if flag === Val(true)\n        # y = compute action of adjoint map on x\n    else\n        # y = compute action of linear map on x\n    end\n    return y\nend\n\nFor a general linear map encoded using either the tuple or the two-argument form, the best approach is to provide a start vector x₀ (in the codomain, i.e. column space, of the linear map). Alternatively, one can specify the number m of rows of the linear map, in which case x₀ = rand(T, m) is used, where the default value of T is Float64, unless specified differently. If an AbstractMatrix is used, a starting vector x₀ does not need to be provided; it is chosen as rand(T, size(A, 1)).\n\nThe next arguments are optional, but should typically be specified. howmany specifies how many singular values and vectors should be computed; which specifies which singular values should be targeted. Valid specifications of which are\n\nLR: largest singular values\nSR: smallest singular values However, the largest singular values tend to converge more rapidly.\n\nReturn values:\n\nThe return value is always of the form vals, lvecs, rvecs, info = svdsolve(...) with\n\nvals: a Vector{<:Real} containing the singular values, of length at least howmany, but could be longer if more singular values were converged at the same cost.\nlvecs: a Vector of corresponding left singular vectors, of the same length as vals.\nrvecs: a Vector of corresponding right singular vectors, of the same length as vals. Note that singular vectors are not returned as a matrix, as the linear map could act on any custom Julia type with vector like behavior, i.e. the elements of the\n\nlists lvecs(rvecs) are objects that are typically similar to the starting guess x₀(A' * x₀), up to a possibly different eltype. When the linear map is a simple     AbstractMatrix, lvecs and rvecs will be Vector{Vector{<:Number}}.\n\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: indicates how many singular values and vectors were actually converged to the specified tolerance tol (see below under keyword arguments)\ninfo.residual::Vector: a list of the same length as vals containing the residuals info.residual[i] = A * rvecs[i] - vals[i] * lvecs[i].\ninfo.normres::Vector{<:Real}: list of the same length as vals containing the norm of the residual info.normres[i] = norm(info.residual[i])\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A or A'.\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if not all requested singular values were converged, so always check if info.converged >= howmany.\n\nKeyword arguments:\n\nKeyword arguments and their default values are given by:\n\nverbosity::Int = 0: verbosity level\n0 (suppress all messages)\n1 (only warnings)\n2 (one message with convergence info at the end)\n3 (progress info after every iteration)\n4+ (all of the above and additional information about the GKL iteration)\nkrylovdim: the maximum dimension of the Krylov subspace that will be constructed. Note that the dimension of the vector space is not known or checked, e.g. x₀ should not necessarily support the Base.length function. If you know the actual problem dimension is smaller than the default value, it is useful to reduce the value of krylovdim, though in principle this should be detected.\ntol: the requested accuracy according to normres as defined above. If you work in e.g. single precision (Float32), you should definitely change the default value.\nmaxiter: the number of times the Krylov subspace can be rebuilt; see below for further details on the algorithms.\north: the orthogonalization method to be used, see Orthogonalizer\neager::Bool = false: if true, eagerly compute the SVD after every expansion of the Krylov subspace to test for convergence, otherwise wait until the Krylov subspace has dimension krylovdim\n\nThe final keyword argument alg_rrule is relevant only when svdsolve is used in a setting where reverse-mode automatic differentation will be used. A custom ChainRulesCore.rrule is defined for svdsolve, which can be evaluated using different algorithms that can be specified via alg_rrule. A suitable default is chosen, so this keyword argument should only be used when this default choice is failing or not performing efficiently. Check the documentation for more information on the possible values for alg_rrule and their implications on the algorithm being used.\n\nAlgorithm\n\nThe last method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here the algorithm is specified, though currently only GKL is available. GKL refers to the the partial Golub-Kahan-Lanczos bidiagonalization which forms the basis for computing the approximation to the singular values. This factorization is dynamically shrunk and expanded (i.e. thick restart) similar to the Krylov-Schur factorization for eigenvalues.\n\n\n\n\n\n","category":"function"},{"location":"man/svd/#Automatic-differentation","page":"Singular value problems","title":"Automatic differentation","text":"","category":"section"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"The svdsolve routine can be used in conjunction with reverse-mode automatic differentiation,  using AD engines that are compatible with the ChainRules ecosystem. The adjoint problem of a singular value problem contains a linear problem, although it can also be formulated as an eigenvalue problem. Details about this approach will be published in a forthcoming manuscript.","category":"page"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"Both svdsolve and the adjoint problem associated with it require the action of the linear map as well as of its adjoint[1]. Hence, no new information about the linear map is required for the adjoint problem. However, the linear map is the only argument that affects the svdsolve output (from a theoretical perspective, the starting vector and algorithm parameters should have no effect), so that this is where the adjoint variables need to be propagated to.","category":"page"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"The adjoint problem (also referred to as cotangent problem) can thus be solved as a linear problem or as an eigenvalue problem. Note that this eigenvalue problem is never symmetric or Hermitian. The different implementations of the rrule can be selected using the alg_rrule keyword argument.  If a linear solver such as GMRES or BiCGStab is specified, the adjoint problem requires solving a] number of linear problems equal to the number of requested singular values and vectors. If an  eigenvalue solver is specified, for which Arnoldi is essentially the only option, then the adjoint problem is solved as a single (but larger) eigenvalue problem.","category":"page"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"Note that the common pair of left and right singular vectors has an arbitrary phase freedom. Hence, a well-defined cost function constructed from singular should depend on these in such a way  that its value is not affected by simultaneously changing the left and right singular vector with a common phase factor, i.e. the cost function should be 'gauge invariant'. If this is not the case,  the cost function is said to be 'gauge dependent', and this can be detected in the resulting adjoint variables for those singular vectors. The KrylovKit rrule for svdsolve will print a warning if it detects from the incoming adjoint variables that the cost function is gauge dependent. This warning can be suppressed by passing alg_rrule an algorithm with verbosity=-1.","category":"page"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"[1]: For a linear map, the adjoint or pullback required in the reverse-order chain rule coincides","category":"page"},{"location":"man/svd/","page":"Singular value problems","title":"Singular value problems","text":"with its (conjugate) transpose, at least with respect to the standard Euclidean inner product.","category":"page"},{"location":"man/implementation/#Implementation-details","page":"Implementation details","title":"Implementation details","text":"","category":"section"},{"location":"man/implementation/#Orthogonalization","page":"Implementation details","title":"Orthogonalization","text":"","category":"section"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"To denote a basis of vectors, e.g. to represent a given Krylov subspace, there is an abstract type Basis{T}","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.Basis","category":"page"},{"location":"man/implementation/#KrylovKit.Basis","page":"Implementation details","title":"KrylovKit.Basis","text":"abstract type Basis{T} end\n\nAn abstract type to collect specific types for representing a basis of vectors of type T.\n\nImplementations of Basis{T} behave in many ways like Vector{T} and should have a length, can be indexed (getindex and setindex!), iterated over (iterate), and support resizing (resize!, pop!, push!, empty!, sizehint!).\n\nThe type T denotes the type of the elements stored in an Basis{T} and can be any custom type that has vector like behavior (as defined in the docs of KrylovKit).\n\nSee OrthonormalBasis for a specific implementation.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"Many Krylov based algorithms use an orthogonal basis to parameterize the Krylov subspace. In that case, the specific implementation OrthonormalBasis{T} can be used:","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.OrthonormalBasis","category":"page"},{"location":"man/implementation/#KrylovKit.OrthonormalBasis","page":"Implementation details","title":"KrylovKit.OrthonormalBasis","text":"OrthonormalBasis{T} <: Basis{T}\n\nA list of vector like objects of type T that are mutually orthogonal and normalized to one, representing an orthonormal basis for some subspace (typically a Krylov subspace). See also Basis\n\nOrthonormality of the vectors contained in an instance b of OrthonormalBasis (i.e. all(inner(b[i],b[j]) == I[i,j] for i=1:length(b), j=1:length(b))) is not checked when elements are added; it is up to the algorithm that constructs b to guarantee orthonormality.\n\nOne can easily orthogonalize or orthonormalize a given vector v with respect to a b::OrthonormalBasis using the functions w, = orthogonalize(v,b,...) or w, = orthonormalize(v,b,...). The resulting vector w of the latter can then be added to b using push!(b, w). Note that in place versions orthogonalize!!(v, b, ...) or orthonormalize!!(v, b, ...) are also available.\n\nFinally, a linear combination of the vectors in b::OrthonormalBasis can be obtained by multiplying b with a Vector{<:Number} using * or mul! (if the output vector is already allocated).\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"We can orthogonalize or orthonormalize a given vector to another vector (assumed normalized) or to a given KrylovKit.OrthonormalBasis using","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.orthogonalize\nKrylovKit.orthonormalize","category":"page"},{"location":"man/implementation/#KrylovKit.orthogonalize","page":"Implementation details","title":"KrylovKit.orthogonalize","text":"orthogonalize(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, x\northogonalize!!(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, x\n\northogonalize(v, q, algorithm::Orthogonalizer]) -> w, s\northogonalize!!(v, q, algorithm::Orthogonalizer]) -> w, s\n\nOrthogonalize vector v against all the vectors in the orthonormal basis b using the orthogonalization algorithm alg of type Orthogonalizer, and return the resulting vector w and the overlap coefficients x of v with the basis vectors in b.\n\nIn case of orthogonalize!, the vector v is mutated in place. In both functions, storage for the overlap coefficients x can be provided as optional argument x::AbstractVector with length(x) >= length(b).\n\nOne can also orthogonalize v against a given vector q (assumed to be normalized), in which case the orthogonal vector w and the inner product s between v and q are returned.\n\nNote that w is not normalized, see also orthonormalize.\n\nFor more information on possible orthogonalization algorithms, see Orthogonalizer and its concrete subtypes ClassicalGramSchmidt, ModifiedGramSchmidt, ClassicalGramSchmidt2, ModifiedGramSchmidt2, ClassicalGramSchmidtIR and ModifiedGramSchmidtIR.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.orthonormalize","page":"Implementation details","title":"KrylovKit.orthonormalize","text":"orthonormalize(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, β, x\northonormalize!!(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, β, x\n\northonormalize(v, q, algorithm::Orthogonalizer]) -> w, β, s\northonormalize!!(v, q, algorithm::Orthogonalizer]) -> w, β, s\n\nOrthonormalize vector v against all the vectors in the orthonormal basis b using the orthogonalization algorithm alg of type Orthogonalizer, and return the resulting vector w (of norm 1), its norm β after orthogonalizing and the overlap coefficients x of v with the basis vectors in b, such that v = β * w + b * x.\n\nIn case of orthogonalize!, the vector v is mutated in place. In both functions, storage for the overlap coefficients x can be provided as optional argument x::AbstractVector with length(x) >= length(b).\n\nOne can also orthonormalize v against a given vector q (assumed to be normalized), in which case the orthonormal vector w, its norm β before normalizing and the inner product s between v and q are returned.\n\nSee orthogonalize if w does not need to be normalized.\n\nFor more information on possible orthogonalization algorithms, see Orthogonalizer and its concrete subtypes ClassicalGramSchmidt, ModifiedGramSchmidt, ClassicalGramSchmidt2, ModifiedGramSchmidt2, ClassicalGramSchmidtIR and ModifiedGramSchmidtIR.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"or using the possibly in-place versions","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.orthogonalize!!\nKrylovKit.orthonormalize!!","category":"page"},{"location":"man/implementation/#KrylovKit.orthogonalize!!","page":"Implementation details","title":"KrylovKit.orthogonalize!!","text":"orthogonalize(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, x\northogonalize!!(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, x\n\northogonalize(v, q, algorithm::Orthogonalizer]) -> w, s\northogonalize!!(v, q, algorithm::Orthogonalizer]) -> w, s\n\nOrthogonalize vector v against all the vectors in the orthonormal basis b using the orthogonalization algorithm alg of type Orthogonalizer, and return the resulting vector w and the overlap coefficients x of v with the basis vectors in b.\n\nIn case of orthogonalize!, the vector v is mutated in place. In both functions, storage for the overlap coefficients x can be provided as optional argument x::AbstractVector with length(x) >= length(b).\n\nOne can also orthogonalize v against a given vector q (assumed to be normalized), in which case the orthogonal vector w and the inner product s between v and q are returned.\n\nNote that w is not normalized, see also orthonormalize.\n\nFor more information on possible orthogonalization algorithms, see Orthogonalizer and its concrete subtypes ClassicalGramSchmidt, ModifiedGramSchmidt, ClassicalGramSchmidt2, ModifiedGramSchmidt2, ClassicalGramSchmidtIR and ModifiedGramSchmidtIR.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.orthonormalize!!","page":"Implementation details","title":"KrylovKit.orthonormalize!!","text":"orthonormalize(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, β, x\northonormalize!!(v, b::OrthonormalBasis, [x::AbstractVector,] alg::Orthogonalizer]) -> w, β, x\n\northonormalize(v, q, algorithm::Orthogonalizer]) -> w, β, s\northonormalize!!(v, q, algorithm::Orthogonalizer]) -> w, β, s\n\nOrthonormalize vector v against all the vectors in the orthonormal basis b using the orthogonalization algorithm alg of type Orthogonalizer, and return the resulting vector w (of norm 1), its norm β after orthogonalizing and the overlap coefficients x of v with the basis vectors in b, such that v = β * w + b * x.\n\nIn case of orthogonalize!, the vector v is mutated in place. In both functions, storage for the overlap coefficients x can be provided as optional argument x::AbstractVector with length(x) >= length(b).\n\nOne can also orthonormalize v against a given vector q (assumed to be normalized), in which case the orthonormal vector w, its norm β before normalizing and the inner product s between v and q are returned.\n\nSee orthogonalize if w does not need to be normalized.\n\nFor more information on possible orthogonalization algorithms, see Orthogonalizer and its concrete subtypes ClassicalGramSchmidt, ModifiedGramSchmidt, ClassicalGramSchmidt2, ModifiedGramSchmidt2, ClassicalGramSchmidtIR and ModifiedGramSchmidtIR.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"The expansion coefficients of a general vector in terms of a given orthonormal basis can be obtained as","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.project!!","category":"page"},{"location":"man/implementation/#KrylovKit.project!!","page":"Implementation details","title":"KrylovKit.project!!","text":"project!!(y::AbstractVector, b::OrthonormalBasis, x,\n    [α::Number = 1, β::Number = 0, r = Base.OneTo(length(b))])\n\nFor a given orthonormal basis b, compute the expansion coefficients y resulting from projecting the vector x onto the subspace spanned by b; more specifically this computes\n\n    y[j] = β*y[j] + α * inner(b[r[j]], x)\n\nfor all j  r.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"whereas the inverse calculation is obtained as","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.unproject!!","category":"page"},{"location":"man/implementation/#KrylovKit.unproject!!","page":"Implementation details","title":"KrylovKit.unproject!!","text":"unproject!!(y, b::OrthonormalBasis, x::AbstractVector,\n    [α::Number = 1, β::Number = 0, r = Base.OneTo(length(b))])\n\nFor a given orthonormal basis b, reconstruct the vector-like object y that is defined by expansion coefficients with respect to the basis vectors in b in x; more specifically this computes\n\n    y = β*y + α * sum(b[r[i]]*x[i] for i = 1:length(r))\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"An orthonormal basis can be transformed using a rank-1 update using","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.rank1update!","category":"page"},{"location":"man/implementation/#KrylovKit.rank1update!","page":"Implementation details","title":"KrylovKit.rank1update!","text":"rank1update!(b::OrthonormalBasis, y, x::AbstractVector,\n    [α::Number = 1, β::Number = 1, r = Base.OneTo(length(b))])\n\nPerform a rank 1 update of a basis b, i.e. update the basis vectors as\n\n    b[r[i]] = β*b[r[i]] + α * y * conj(x[i])\n\nIt is the user's responsibility to make sure that the result is still an orthonormal basis.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"Note that this changes the subspace. A mere rotation of the basis, which does not change the subspace spanned by it, can be computed using","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.basistransform!","category":"page"},{"location":"man/implementation/#KrylovKit.basistransform!","page":"Implementation details","title":"KrylovKit.basistransform!","text":"basistransform!(b::OrthonormalBasis, U::AbstractMatrix)\n\nTransform the orthonormal basis b by the matrix U. For b an orthonormal basis, the matrix U should be real orthogonal or complex unitary; it is up to the user to ensure this condition is satisfied. The new basis vectors are given by\n\n    b[j] ← b[i] * U[i,j]\n\nand are stored in b, so the old basis vectors are thrown away. Note that, by definition, the subspace spanned by these basis vectors is exactly the same.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#Dense-linear-algebra","page":"Implementation details","title":"Dense linear algebra","text":"","category":"section"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit relies on Julia's LinearAlgebra module from the standard library for most of its dense linear algebra dependencies.","category":"page"},{"location":"man/implementation/#Factorization-types","page":"Implementation details","title":"Factorization types","text":"","category":"section"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"The central ingredient in a Krylov based algorithm is a Krylov factorization or decomposition of a linear map. Such partial factorizations are represented as a KrylovFactorization, of which LanczosFactorization and ArnoldiFactorization are two concrete implementations:","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.KrylovFactorization\nKrylovKit.LanczosFactorization\nKrylovKit.ArnoldiFactorization\nKrylovKit.GKLFactorization","category":"page"},{"location":"man/implementation/#KrylovKit.KrylovFactorization","page":"Implementation details","title":"KrylovKit.KrylovFactorization","text":"abstract type KrylovFactorization{T,S<:Number}\n\nAbstract type to store a Krylov factorization of a linear map A of the form\n\nA * V = V * B + r * b'\n\nFor a given Krylov factorization fact of length k = length(fact), the basis V is obtained via basis(fact) and is an instance of some subtype of Basis{T}, with also length(V) == k and where T denotes the type of vector like objects used in the problem. The Rayleigh quotient B is obtained as rayleighquotient(fact) and typeof(B) is some subtype of AbstractMatrix{S} with size(B) == (k,k), typically a structured matrix. The residual r is obtained as residual(fact) and is of type T. One can also query normres(fact) to obtain norm(r), the norm of the residual. The vector b has no dedicated name and often takes a default form (see below). It should be a subtype of AbstractVector of length k and can be obtained as rayleighextension(fact) (by lack of a better dedicated name).\n\nA Krylov factorization fact can be destructured as V, B, r, nr, b = fact with nr = norm(r).\n\nSee also LanczosFactorization and ArnoldiFactorization for concrete implementations, and KrylovIterator (with in particular LanczosIterator and ArnoldiIterator) for iterators that construct progressively expanding Krylov factorizations of a given linear map and a starting vector.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/#KrylovKit.LanczosFactorization","page":"Implementation details","title":"KrylovKit.LanczosFactorization","text":"mutable struct LanczosFactorization{T,S<:Real} <: KrylovFactorization{T,S}\n\nStructure to store a Lanczos factorization of a real symmetric or complex hermitian linear map A of the form\n\nA * V = V * B + r * b'\n\nFor a given Lanczos factorization fact of length k = length(fact), the basis V is obtained via basis(fact) and is an instance of OrthonormalBasis{T}, with also length(V) == k and where T denotes the type of vector like objects used in the problem. The Rayleigh quotient B is obtained as rayleighquotient(fact) and is of type SymTridiagonal{S<:Real} with size(B) == (k,k). The residual r is obtained as residual(fact) and is of type T. One can also query normres(fact) to obtain norm(r), the norm of the residual. The vector b has no dedicated name but can be obtained via rayleighextension(fact). It takes the default value e_k, i.e. the unit vector of all zeros and a one in the last entry, which is represented using SimpleBasisVector.\n\nA Lanczos factorization fact can be destructured as V, B, r, nr, b = fact with nr = norm(r).\n\nLanczosFactorization is mutable because it can expand! or shrink!. See also LanczosIterator for an iterator that constructs a progressively expanding Lanczos factorizations of a given linear map and a starting vector. See ArnoldiFactorization and ArnoldiIterator for a Krylov factorization that works for general (non-symmetric) linear maps.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/#KrylovKit.ArnoldiFactorization","page":"Implementation details","title":"KrylovKit.ArnoldiFactorization","text":"mutable struct ArnoldiFactorization{T,S} <: KrylovFactorization{T,S}\n\nStructure to store an Arnoldi factorization of a linear map A of the form\n\nA * V = V * B + r * b'\n\nFor a given Arnoldi factorization fact of length k = length(fact), the basis V is obtained via basis(fact) and is an instance of OrthonormalBasis{T}, with also length(V) == k and where T denotes the type of vector like objects used in the problem. The Rayleigh quotient B is obtained as rayleighquotient(fact) and is of type B::PackedHessenberg{S<:Number} with size(B) == (k,k). The residual r is obtained as residual(fact) and is of type T. One can also query normres(fact) to obtain norm(r), the norm of the residual. The vector b has no dedicated name but can be obtained via rayleighextension(fact). It takes the default value e_k, i.e. the unit vector of all zeros and a one in the last entry, which is represented using SimpleBasisVector.\n\nAn Arnoldi factorization fact can be destructured as V, B, r, nr, b = fact with nr = norm(r).\n\nArnoldiFactorization is mutable because it can expand! or shrink!. See also ArnoldiIterator for an iterator that constructs a progressively expanding Arnoldi factorizations of a given linear map and a starting vector. See LanczosFactorization and LanczosIterator for a Krylov factorization that is optimized for real symmetric or complex hermitian linear maps.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/#KrylovKit.GKLFactorization","page":"Implementation details","title":"KrylovKit.GKLFactorization","text":"mutable struct GKLFactorization{TU,TV,S<:Real}\n\nStructure to store a Golub-Kahan-Lanczos (GKL) bidiagonal factorization of a linear map A of the form\n\nA * V = U * B + r * b'\nA' * U = V * B'\n\nFor a given GKL factorization fact of length k = length(fact), the two bases U and V are obtained via basis(fact, Val(:U)) and basis(fact, Val(:V)). Here, U and V are instances of OrthonormalBasis{T}, with also length(U) == length(V) == k and where T denotes the type of vector like objects used in the problem. The Rayleigh quotient B is obtained as rayleighquotient(fact) and is of type Bidiagonal{S<:Number} with size(B) == (k,k). The residual r is obtained as residual(fact) and is of type T. One can also query normres(fact) to obtain norm(r), the norm of the residual. The vector b has no dedicated name but can be obtained via rayleighextension(fact). It takes the default value e_k, i.e. the unit vector of all zeros and a one in the last entry, which is represented using SimpleBasisVector.\n\nA GKL factorization fact can be destructured as U, V, B, r, nr, b = fact with nr = norm(r).\n\nGKLFactorization is mutable because it can expand! or shrink!. See also GKLIterator for an iterator that constructs a progressively expanding GKL factorizations of a given linear map and a starting vector u₀.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"A KrylovFactorization or GKLFactorization can be destructured into its defining components using iteration, but these can also be accessed using the following functions","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"basis\nrayleighquotient\nresidual\nnormres\nrayleighextension","category":"page"},{"location":"man/implementation/#KrylovKit.basis","page":"Implementation details","title":"KrylovKit.basis","text":"    basis(fact::KrylovFactorization)\n\nReturn the list of basis vectors of a KrylovFactorization, which span the Krylov subspace. The return type is a subtype of Basis{T}, where T represents the type of the vectors used by the problem.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.rayleighquotient","page":"Implementation details","title":"KrylovKit.rayleighquotient","text":"rayleighquotient(fact::KrylovFactorization)\n\nReturn the Rayleigh quotient of a KrylovFactorization, i.e. the reduced matrix within the basis of the Krylov subspace. The return type is a subtype of AbstractMatrix{<:Number}, typically some structured matrix type.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.residual","page":"Implementation details","title":"KrylovKit.residual","text":"residual(fact::KrylovFactorization)\n\nReturn the residual of a KrylovFactorization. The return type is some vector of the same type as used in the problem. See also normres(F) for its norm, which typically has been computed already.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.normres","page":"Implementation details","title":"KrylovKit.normres","text":"normres(fact::KrylovFactorization)\n\nReturn the norm of the residual of a KrylovFactorization. As this has typically already been computed, it is cheaper than (but otherwise equivalent to) norm(residual(F)).\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.rayleighextension","page":"Implementation details","title":"KrylovKit.rayleighextension","text":"rayleighextension(fact::KrylovFactorization)\n\nReturn the vector b appearing in the definition of a KrylovFactorization; often it is simply the last coordinate unit vector, which can be represented using SimpleBasisVector.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"As the rayleighextension is typically a simple basis vector, we have created a dedicated type to represent this without having to allocate an actual vector, i.e.","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.SimpleBasisVector","category":"page"},{"location":"man/implementation/#KrylovKit.SimpleBasisVector","page":"Implementation details","title":"KrylovKit.SimpleBasisVector","text":"SimpleBasisVector(m, k)\n\nConstruct a simple struct SimpleBasisVector <: AbstractVector{Bool} representing a coordinate basis vector of length m in the direction of k, i.e. for e_k = SimpleBasisVector(m, k) we have length(e_k) = m and e_k[i] = (i == k).\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"Furthermore, to store the Rayleigh quotient of the Arnoldi factorization in a manner that can easily be expanded, we have constructed a custom matrix type to store the Hessenberg matrix in a packed format (without zeros):","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.PackedHessenberg","category":"page"},{"location":"man/implementation/#KrylovKit.PackedHessenberg","page":"Implementation details","title":"KrylovKit.PackedHessenberg","text":"struct PackedHessenberg{T,V<:AbstractVector{T}} <: AbstractMatrix{T}\n    data::V\n    n::Int\nend\n\nA custom struct to store a Hessenberg matrix in a packed format (without zeros). Hereto, the non-zero entries are stored sequentially in vector data of length n(n+1)/2.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/#Factorization-iterators","page":"Implementation details","title":"Factorization iterators","text":"","category":"section"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"Given a linear map A and a starting vector x₀, a Krylov factorization is obtained by sequentially building a Krylov subspace x₀ A x₀ A² x₀ . Rather then using this set of vectors as a basis, an orthonormal basis is generated by a process known as Lanczos or Arnoldi iteration (for symmetric/hermitian and for general matrices, respectively). These processes are represented as iterators in Julia:","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.KrylovIterator\nKrylovKit.LanczosIterator\nKrylovKit.ArnoldiIterator","category":"page"},{"location":"man/implementation/#KrylovKit.KrylovIterator","page":"Implementation details","title":"KrylovKit.KrylovIterator","text":"abstract type KrylovIterator{F,T}\n\nAbstract type for iterators that take a linear map of type F and an initial vector of type T and generate an expanding KrylovFactorization thereof.\n\nWhen iterating over an instance of KrylovIterator, the values being generated are subtypes of KrylovFactorization, which can be immediately destructured into a basis, rayleighquotient, residual, normres and rayleighextension.\n\nSee LanczosIterator and ArnoldiIterator for concrete implementations and more information.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/#KrylovKit.LanczosIterator","page":"Implementation details","title":"KrylovKit.LanczosIterator","text":"struct LanczosIterator{F,T,O<:Orthogonalizer} <: KrylovIterator{F,T}\nLanczosIterator(f, v₀, [orth::Orthogonalizer = KrylovDefaults.orth, keepvecs::Bool = true])\n\nIterator that takes a linear map f::F (supposed to be real symmetric or complex hermitian) and an initial vector v₀::T and generates an expanding LanczosFactorization thereof. In particular, LanczosIterator uses the Lanczos iteration scheme to build a successively expanding Lanczos factorization. While f cannot be tested to be symmetric or hermitian directly when the linear map is encoded as a general callable object or function, it is tested whether the imaginary part of inner(v, f(v)) is sufficiently small to be neglected.\n\nThe argument f can be a matrix, or a function accepting a single argument v, so that f(v) implements the action of the linear map on the vector v.\n\nThe optional argument orth specifies which Orthogonalizer to be used. The default value in KrylovDefaults is to use ModifiedGramSchmidtIR, which possibly uses reorthogonalization steps. One can use to discard the old vectors that span the Krylov subspace by setting the final argument keepvecs to false. This, however, is only possible if an orth algorithm is used that does not rely on reorthogonalization, such as ClassicalGramSchmidt() or ModifiedGramSchmidt(). In that case, the iterator strictly uses the Lanczos three-term recurrence relation.\n\nWhen iterating over an instance of LanczosIterator, the values being generated are instances of LanczosFactorization, which can be immediately destructured into a basis, rayleighquotient, residual, normres and rayleighextension, for example as\n\nfor (V, B, r, nr, b) in LanczosIterator(f, v₀)\n    # do something\n    nr < tol && break # a typical stopping criterion\nend\n\nNote, however, that if keepvecs=false in LanczosIterator, the basis V cannot be extracted.\n\nSince the iterator does not know the dimension of the underlying vector space of objects of type T, it keeps expanding the Krylov subspace until the residual norm nr falls below machine precision eps(typeof(nr)).\n\nThe internal state of LanczosIterator is the same as the return value, i.e. the corresponding LanczosFactorization. However, as Julia's Base iteration interface (using Base.iterate) requires that the state is not mutated, a deepcopy is produced upon every next iteration step.\n\nInstead, you can also mutate the KrylovFactorization in place, using the following interface, e.g. for the same example above\n\niterator = LanczosIterator(f, v₀)\nfactorization = initialize(iterator)\nwhile normres(factorization) > tol\n    expand!(iterator, factorization)\n    V, B, r, nr, b = factorization\n    # do something\nend\n\nHere, initialize(::KrylovIterator) produces the first Krylov factorization of length 1, and expand!(::KrylovIterator, ::KrylovFactorization)(@ref) expands the factorization in place. See also initialize!(::KrylovIterator, ::KrylovFactorization) to initialize in an already existing factorization (most information will be discarded) and shrink!(::KrylovFactorization, k) to shrink an existing factorization down to length k.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/#KrylovKit.ArnoldiIterator","page":"Implementation details","title":"KrylovKit.ArnoldiIterator","text":"struct ArnoldiIterator{F,T,O<:Orthogonalizer} <: KrylovIterator{F,T}\nArnoldiIterator(f, v₀, [orth::Orthogonalizer = KrylovDefaults.orth])\n\nIterator that takes a general linear map f::F and an initial vector v₀::T and generates an expanding ArnoldiFactorization thereof. In particular, ArnoldiIterator iterates over progressively expanding Arnoldi factorizations using the Arnoldi iteration.\n\nThe argument f can be a matrix, or a function accepting a single argument v, so that f(v) implements the action of the linear map on the vector v.\n\nThe optional argument orth specifies which Orthogonalizer to be used. The default value in KrylovDefaults is to use ModifiedGramSchmidtIR, which possibly uses reorthogonalization steps.\n\nWhen iterating over an instance of ArnoldiIterator, the values being generated are instances of ArnoldiFactorization, which can be immediately destructured into a basis, rayleighquotient, residual, normres and rayleighextension, for example as\n\nfor (V, B, r, nr, b) in ArnoldiIterator(f, v₀)\n    # do something\n    nr < tol && break # a typical stopping criterion\nend\n\nSince the iterator does not know the dimension of the underlying vector space of objects of type T, it keeps expanding the Krylov subspace until the residual norm nr falls below machine precision eps(typeof(nr)).\n\nThe internal state of ArnoldiIterator is the same as the return value, i.e. the corresponding ArnoldiFactorization. However, as Julia's Base iteration interface (using Base.iterate) requires that the state is not mutated, a deepcopy is produced upon every next iteration step.\n\nInstead, you can also mutate the ArnoldiFactorization in place, using the following interface, e.g. for the same example above\n\niterator = ArnoldiIterator(f, v₀)\nfactorization = initialize(iterator)\nwhile normres(factorization) > tol\n    expand!(iterator, factorization)\n    V, B, r, nr, b = factorization\n    # do something\nend\n\nHere, initialize(::KrylovIterator) produces the first Krylov factorization of length 1, and expand!(::KrylovIterator, ::KrylovFactorization)(@ref) expands the factorization in place. See also initialize!(::KrylovIterator, ::KrylovFactorization) to initialize in an already existing factorization (most information will be discarded) and shrink!(::KrylovFactorization, k) to shrink an existing factorization down to length k.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"Similarly, there is also an iterator for the Golub-Kahan-Lanczos bidiagonalization proces:","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"KrylovKit.GKLIterator","category":"page"},{"location":"man/implementation/#KrylovKit.GKLIterator","page":"Implementation details","title":"KrylovKit.GKLIterator","text":"struct GKLIterator{F,TU,O<:Orthogonalizer}\nGKLIterator(f, u₀, [orth::Orthogonalizer = KrylovDefaults.orth, keepvecs::Bool = true])\n\nIterator that takes a general linear map f::F and an initial vector u₀::TU and generates an expanding GKLFactorization thereof. In particular, GKLIterator implements the Golub-Kahan-Lanczos bidiagonalization procedure. Note, however, that this implementation starts from a vector u₀ in the codomain of the linear map f, which will end up (after normalisation) as the first column of U.\n\nThe argument f can be a matrix, a tuple of two functions where the first represents the normal action and the second the adjoint action, or a function accepting two arguments, where the first argument is the vector to which the linear map needs to be applied, and the second argument is either Val(false) for the normal action and Val(true) for the adjoint action. Note that the flag is thus a Val type to allow for type stability in cases where the vectors in the domain and the codomain of the linear map have a different type.\n\nThe optional argument orth specifies which Orthogonalizer to be used. The default value in KrylovDefaults is to use ModifiedGramSchmidtIR, which possibly uses reorthogonalization steps.\n\nWhen iterating over an instance of GKLIterator, the values being generated are instances fact of GKLFactorization, which can be immediately destructured into a basis(fact, Val(:U)), basis(fact, Val(:V)), rayleighquotient, residual, normres and rayleighextension, for example as\n\nfor (U, V, B, r, nr, b) in GKLIterator(f, u₀)\n    # do something\n    nr < tol && break # a typical stopping criterion\nend\n\nSince the iterator does not know the dimension of the underlying vector space of objects of type T, it keeps expanding the Krylov subspace until the residual norm nr falls below machine precision eps(typeof(nr)).\n\nThe internal state of GKLIterator is the same as the return value, i.e. the corresponding GKLFactorization. However, as Julia's Base iteration interface (using Base.iterate) requires that the state is not mutated, a deepcopy is produced upon every next iteration step.\n\nInstead, you can also mutate the GKLFactorization in place, using the following interface, e.g. for the same example above\n\niterator = GKLIterator(f, u₀)\nfactorization = initialize(iterator)\nwhile normres(factorization) > tol\n    expand!(iterator, factorization)\n    U, V, B, r, nr, b = factorization\n    # do something\nend\n\nHere, initialize(::GKLIterator) produces the first GKL factorization of length 1, and expand!(::GKLIterator, ::GKLFactorization)(@ref) expands the factorization in place. See also initialize!(::GKLIterator, ::GKLFactorization) to initialize in an already existing factorization (most information will be discarded) and shrink!(::GKLIterator, k) to shrink an existing factorization down to length k.\n\n\n\n\n\n","category":"type"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"As an alternative to the standard iteration interface from Julia Base (using iterate), these iterative processes and the factorizations they produce can also be manipulated using the following functions:","category":"page"},{"location":"man/implementation/","page":"Implementation details","title":"Implementation details","text":"expand!\nshrink!\ninitialize\ninitialize!","category":"page"},{"location":"man/implementation/#KrylovKit.expand!","page":"Implementation details","title":"KrylovKit.expand!","text":"expand!(iter::KrylovIterator, fact::KrylovFactorization)\n\nExpand the Krylov factorization fact by one using the linear map and parameters in iter.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.shrink!","page":"Implementation details","title":"KrylovKit.shrink!","text":"shrink!(fact::KrylovFactorization, k)\n\nShrink an existing Krylov factorization fact down to have length k. Does nothing if length(fact)<=k.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.initialize","page":"Implementation details","title":"KrylovKit.initialize","text":"initialize(iter::KrylovIterator)\n\nInitialize a length 1 Krylov factorization corresponding to iter.\n\n\n\n\n\n","category":"function"},{"location":"man/implementation/#KrylovKit.initialize!","page":"Implementation details","title":"KrylovKit.initialize!","text":"initialize!(iter::KrylovIterator, fact::KrylovFactorization)\n\nInitialize a length 1 Krylov factorization corresponding to iter in the already existing factorization fact, thereby destroying all the information it currently holds.\n\n\n\n\n\n","category":"function"},{"location":"man/eig/#Eigenvalue-problems","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"","category":"section"},{"location":"man/eig/#Eigenvalues-and-eigenvectors","page":"Eigenvalue problems","title":"Eigenvalues and eigenvectors","text":"","category":"section"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Finding a selection of eigenvalues and corresponding (right) eigenvectors of a linear map can be accomplished with the eigsolve routine:","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"eigsolve","category":"page"},{"location":"man/eig/#KrylovKit.eigsolve","page":"Eigenvalue problems","title":"KrylovKit.eigsolve","text":"eigsolve(A::AbstractMatrix, [x₀, howmany = 1, which = :LM, T = eltype(A)]; kwargs...)\neigsolve(f, n::Int, [howmany = 1, which = :LM, T = Float64]; kwargs...)\neigsolve(f, x₀, [howmany = 1, which = :LM]; kwargs...)\n# expert version:\neigsolve(f, x₀, howmany, which, algorithm; alg_rrule=...)\n\nCompute at least howmany eigenvalues from the linear map encoded in the matrix A or by the function f. Return eigenvalues, eigenvectors and a ConvergenceInfo structure.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object. If an AbstractMatrix is used, a starting vector x₀ does not need to be provided, it is then chosen as rand(T, size(A, 1)). If the linear map is encoded more generally as a a callable function or method, the best approach is to provide an explicit starting guess x₀. Note that x₀ does not need to be of type AbstractVector; any type that behaves as a vector and supports the required methods (see KrylovKit docs) is accepted. If instead of x₀ an integer n is specified, it is assumed that x₀ is a regular vector and it is initialized to rand(T, n), where the default value of T is Float64, unless specified differently.\n\nThe next arguments are optional, but should typically be specified. howmany specifies how many eigenvalues should be computed; which specifies which eigenvalues should be targeted. Valid specifications of which are given by\n\n:LM: eigenvalues of largest magnitude\n:LR: eigenvalues with largest (most positive) real part\n:SR: eigenvalues with smallest (most negative) real part\n:LI: eigenvalues with largest (most positive) imaginary part, only if T <: Complex\n:SI: eigenvalues with smallest (most negative) imaginary part, only if T <: Complex\nEigSorter(f; rev = false): eigenvalues λ that appear first (or last if rev == true) when sorted by f(λ)\n\nnote: Note about selecting `which` eigenvalues\nKrylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. All of the valid Symbols for which have this property, but could also be specified using EigSorter, e.g. :LM is equivalent to Eigsorter(abs; rev = true). Note that smallest magnitude sorting is obtained using e.g. EigSorter(abs; rev = false), but since no (shift-and)-invert is used, this will only be successful if you somehow know that eigenvalues close to zero are also close to the periphery of the spectrum.\n\nwarning: Degenerate eigenvalues\nFrom a theoretical point of view, Krylov methods can at most find a single eigenvector associated with a targetted eigenvalue, even if the latter is degenerate. In the case of a degenerate eigenvalue, the specific eigenvector that is returned is determined by the starting vector x₀. For large problems, this turns out to be less of an issue in practice, as often a second linearly independent eigenvector is generated out of the numerical noise resulting from the orthogonalisation steps in the Lanczos or Arnoldi iteration. Nonetheless, it is important to take this into account and to try not to depend on this potentially fragile behaviour, especially for smaller problems.\n\nThe argument T acts as a hint in which Number type the computation should be performed, but is not restrictive. If the linear map automatically produces complex values, complex arithmetic will be used even though T<:Real was specified. However, if the linear map and initial guess are real, approximate eigenvalues will be searched for using a partial Schur factorization, which implies that complex conjugate eigenvalues come in pairs and cannot be split. It is then illegal to choose which in a way that would treat λ and conj(λ) differently, i.e. :LI and :SI are invalid, as well as any EigSorter that would lead to by(λ) != by(conj(λ)).\n\nReturn values:\n\nThe return value is always of the form vals, vecs, info = eigsolve(...) with\n\nvals: a Vector containing the eigenvalues, of length at least howmany, but could be longer if more eigenvalues were converged at the same cost. Eigenvalues will be real if Lanczos was used and complex if Arnoldi was used (see below).\nvecs: a Vector of corresponding eigenvectors, of the same length as vals. Note that eigenvectors are not returned as a matrix, as the linear map could act on any custom Julia type with vector like behavior, i.e. the elements of the list vecs are objects that are typically similar to the starting guess x₀, up to a possibly different eltype. In particular  for a general matrix (i.e. with Arnoldi) the eigenvectors are generally complex and are therefore always returned in a complex number format. When the linear map is a simple AbstractMatrix, vecs will be Vector{Vector{<:Number}}.\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: indicates how many eigenvalues and eigenvectors were actually converged to the specified tolerance tol (see below under keyword arguments)\ninfo.residual::Vector: a list of the same length as vals containing the residuals info.residual[i] = f(vecs[i]) - vals[i] * vecs[i]\ninfo.normres::Vector{<:Real}: list of the same length as vals containing the norm of the residual info.normres[i] = norm(info.residual[i])\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if not all requested eigenvalues were converged, so always check if info.converged >= howmany.\n\nKeyword arguments:\n\nKeyword arguments and their default values are given by:\n\nverbosity::Int = 0: verbosity level, i.e. \n0 (suppress all messages)\n1 (only warnings)\n2 (one message with convergence info at the end)\n3 (progress info after every iteration)\n4+ (all of the above and additional information about the Lanczos or Arnoldi iteration)\ntol::Real: the requested accuracy (corresponding to the 2-norm of the residual for Schur vectors, not the eigenvectors). If you work in e.g. single precision (Float32), you should definitely change the default value.\nkrylovdim::Integer: the maximum dimension of the Krylov subspace that will be constructed. Note that the dimension of the vector space is not known or checked, e.g. x₀ should not necessarily support the Base.length function. If you know the actual problem dimension is smaller than the default value, it is useful to reduce the value of krylovdim, though in principle this should be detected.\nmaxiter::Integer: the number of times the Krylov subspace can be rebuilt; see below for further details on the algorithms.\north::Orthogonalizer: the orthogonalization method to be used, see Orthogonalizer\nissymmetric::Bool: if the linear map is symmetric, only meaningful if T<:Real\nishermitian::Bool: if the linear map is hermitian\neager::Bool = false: if true, eagerly compute the eigenvalue or Schur decomposition after every expansion of the Krylov subspace to test for convergence, otherwise wait until the Krylov subspace has dimension krylovdim. This can result in a faster return, for example if the initial guess is very good, but also has some overhead, as many more dense Schur factorizations need to be computed.\n\nThe default values are given by tol = KrylovDefaults.tol, krylovdim = KrylovDefaults.krylovdim, maxiter = KrylovDefaults.maxiter, orth = KrylovDefaults.orth; see KrylovDefaults for details.\n\nThe default value for the last two parameters depends on the method. If an AbstractMatrix is used, issymmetric and ishermitian are checked for that matrix, otherwise the default values are issymmetric = false and ishermitian = T <: Real && issymmetric. When values for the keyword arguments are provided, no checks will be performed even in the matrix case.\n\nThe final keyword argument alg_rrule is relevant only when eigsolve is used in a setting where reverse-mode automatic differentation will be used. A custom ChainRulesCore.rrule is defined for eigsolve, which can be evaluated using different algorithms that can be specified via alg_rrule. A suitable default is chosen, so this keyword argument should only be used when this default choice is failing or not performing efficiently. Check the documentation for more information on the possible values for alg_rrule and their implications on the algorithm being used.\n\nAlgorithm\n\nThe final (expert) method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here, one specifies the algorithm explicitly as either Lanczos, for real symmetric or complex hermitian problems, or Arnoldi, for general problems. Note that these names refer to the process for building the Krylov subspace, but the actual algorithm is an implementation of the Krylov-Schur algorithm, which can dynamically shrink and grow the Krylov subspace, i.e. the restarts are so-called thick restarts where a part of the current Krylov subspace is kept.\n\nnote: Note about convergence\nIn case of a general problem, where the Arnoldi method is used, convergence of an eigenvalue is not based on the norm of the residual norm(f(vecs[i]) - vals[i]*vecs[i]) for the eigenvector but rather on the norm of the residual for the corresponding Schur vectors.See also schursolve if you want to use the partial Schur decomposition directly, or if you are not interested in computing the eigenvectors, and want to work in real arithmetic all the way true (if the linear map and starting guess are real). If you have knowledge that all requested eigenvalues of a real problem will be real, and thus also their associated eigenvectors, you can also use realeigsolve.\n\n\n\n\n\n","category":"function"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Which eigenvalues are targeted can be specified using one of the symbols :LM, :LR, :SR, :LI and :SI for largest magnitude, largest and smallest real part, and largest and smallest imaginary part respectively. Alternatively, one can just specify a general sorting operation using EigSorter","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"EigSorter","category":"page"},{"location":"man/eig/#KrylovKit.EigSorter","page":"Eigenvalue problems","title":"KrylovKit.EigSorter","text":"EigSorter(by; rev = false)\n\nA simple struct to be used in combination with eigsolve or schursolve to indicate which eigenvalues need to be targeted, namely those that appear first when sorted by by and possibly in reverse order if rev == true.\n\n\n\n\n\n","category":"type"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"For a general matrix, eigenvalues and eigenvectors will always be returned with complex values for reasons of type stability. However, if the linear map and initial guess are real, most of the computation is actually performed using real arithmetic, as in fact the first step is to compute an approximate partial Schur factorization. If one is not interested in the eigenvectors, one can also just compute this partial Schur factorization using schursolve, for which only an 'expert' method call is available","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"schursolve","category":"page"},{"location":"man/eig/#KrylovKit.schursolve","page":"Eigenvalue problems","title":"KrylovKit.schursolve","text":"# expert version:\nschursolve(f, x₀, howmany, which, algorithm)\n\nCompute a partial Schur decomposition containing howmany eigenvalues from the linear map encoded in the matrix or function A. Return the reduced Schur matrix, the basis of Schur vectors, the extracted eigenvalues and a ConvergenceInfo structure.\n\nSee also eigsolve to obtain the eigenvectors instead. For real symmetric or complex hermitian problems, the (partial) Schur decomposition is identical to the (partial) eigenvalue decomposition, and eigsolve should always be used.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object, that acts on vector like objects similar to x₀, which is the starting guess from which a Krylov subspace will be built. howmany specifies how many Schur vectors should be converged before the algorithm terminates; which specifies which eigenvalues should be targeted. Valid specifications of which are\n\nLM: eigenvalues of largest magnitude\nLR: eigenvalues with largest (most positive) real part\nSR: eigenvalues with smallest (most negative) real part\nLI: eigenvalues with largest (most positive) imaginary part, only if T <: Complex\nSI: eigenvalues with smallest (most negative) imaginary part, only if T <: Complex\nEigSorter(f; rev = false): eigenvalues λ that appear first (or last if rev == true) when sorted by f(λ)\n\nnote: Note about selecting `which` eigenvalues\nKrylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. All of they valid Symbols for which have this property, but could also be specified using EigSorter, e.g. :LM is equivalent to Eigsorter(abs; rev = true). Note that smallest magnitude sorting is obtained using e.g. EigSorter(abs; rev = false), but since no (shift-and)-invert is used, this will only be successful if you somehow know that eigenvalues close to zero are also close to the periphery of the spectrum.\n\nwarning: Degenerate eigenvalues\nFrom a theoretical point of view, Krylov methods can at most find a single eigenvector associated with a targetted eigenvalue, even if the latter is degenerate. In the case of a degenerate eigenvalue, the specific eigenvector that is returned is determined by the starting vector x₀. For large problems, this turns out to be less of an issue in practice, as often a second linearly independent eigenvector is generated out of the numerical noise resulting from the orthogonalisation steps in the Lanczos or Arnoldi iteration. Nonetheless, it is important to take this into account and to try not to depend on this potentially fragile behaviour, especially for smaller problems.\n\nThe algorithm argument currently only supports an instance of Arnoldi, which is where the parameters of the Krylov method (such as Krylov dimension and maximum number of iterations) can be specified. Since schursolve is less commonly used as eigsolve, it only supports this expert mode call syntax and no convenient keyword interface is currently available.\n\nReturn values:\n\nThe return value is always of the form T, vecs, vals, info = schursolve(...) with\n\nT: a Matrix containing the partial Schur decomposition of the linear map, i.e. it's elements are given by T[i,j] = inner(vecs[i], f(vecs[j])). It is of Schur form, i.e. upper triangular in case of complex arithmetic, and block upper triangular (with at most 2x2 blocks) in case of real arithmetic.\nvecs: a Vector of corresponding Schur vectors, of the same length as vals. Note that Schur vectors are not returned as a matrix, as the linear map could act on any custom  Julia type with vector like behavior, i.e. the elements of the list vecs are objects that are typically similar to the starting guess x₀, up to a possibly different eltype. When the linear map is a simple AbstractMatrix, vecs will be Vector{Vector{<:Number}}. Schur vectors are by definition orthogonal, i.e. inner(vecs[i],vecs[j]) = I[i,j]. Note that Schur vectors are real if the problem (i.e. the linear map and the initial guess) are real.\nvals: a Vector of eigenvalues, i.e. the diagonal elements of T in case of complex arithmetic, or extracted from the diagonal blocks in case of real arithmetic. Note that vals will always be complex, independent of the underlying arithmetic.\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: indicates how many eigenvalues and Schur vectors were actually converged to the specified tolerance (see below under keyword arguments)\ninfo.residuals::Vector: a list of the same length as vals containing the actual residuals\ninfo.residuals[i] = f(vecs[i]) - sum(vecs[j] * T[j, i] for j in 1:i+1)\nwhere T[i+1,i] is definitely zero in case of complex arithmetic and possibly zero in case of real arithmetic\ninfo.normres::Vector{<:Real}: list of the same length as vals containing the norm of the residual for every Schur vector, i.e. info.normes[i] = norm(info.residual[i])\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if not all requested eigenvalues were converged, so always check if info.converged >= howmany.\n\nAlgorithm\n\nThe actual algorithm is an implementation of the Krylov-Schur algorithm, where the Arnoldi algorithm is used to generate the Krylov subspace. During the algorithm, the Krylov subspace is dynamically grown and shrunk, i.e. the restarts are so-called thick restarts where a part of the current Krylov subspace is kept.\n\n\n\n\n\n","category":"function"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Note that, for symmetric or hermitian linear maps, the eigenvalue and Schur factorization are equivalent, and one should only use eigsolve. There is no schursolve using the Lanczos algorithm.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Another example of a possible use case of schursolve is if the linear map is known to have a unique eigenvalue of, e.g. largest magnitude. Then, if the linear map is real valued, that largest magnitude eigenvalue and its corresponding eigenvector are also real valued. eigsolve will automatically return complex valued eigenvectors for reasons of type stability. However, as the first Schur vector will coincide with the first eigenvector, one can instead use","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"T, vecs, vals, info = schursolve(A, x⁠₀, 1, :LM, Arnoldi(...))","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"and use vecs[1] as the real valued eigenvector (after checking info.converged) corresponding to the largest magnitude eigenvalue of A.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"More generally, if you want to compute several eigenvalues of a real linear map, and you know that all of them are real, so that also the associated eigenvectors will be real, then you can use the realeigsolve method.","category":"page"},{"location":"man/eig/#Automatic-differentation","page":"Eigenvalue problems","title":"Automatic differentation","text":"","category":"section"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"The eigsolve (and realeigsolve) routine can be used in conjunction with reverse-mode automatic  differentiation, using AD engines that are compatible with the ChainRules ecosystem. The adjoint problem of an eigenvalue problem is a linear problem, although it can also be formulated as an eigenvalue problem. Details about this approach will be published in a forthcoming manuscript.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"In either case, the adjoint problem requires the adjoint[1] of the linear map. If the linear map is an AbstractMatrix instance, its adjoint will be used in the rrule. If the linear map is implemented  as a function f, then the AD engine itself is used to compute the corresponding adjoint via  ChainRulesCore.rrule_via_ad(config, f, x). The specific base point x at which this adjoint is computed should not affect the result if f properly represents a linear map. Furthermore, the linear map is the only argument that affects the eigsolve output (from a theoretical perspective, the starting vector and algorithm parameters should have no effect), so that this is where the adjoint  variables need to be propagated to and have a nonzero effect.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"The adjoint problem (also referred to as cotangent problem) can thus be solved as a linear problem or as an eigenvalue problem. Note that this eigenvalue problem is never symmetric or Hermitian, even if the primal problem is. The different implementations of the rrule can be selected using the alg_rrule keyword argument. If a linear solver such as GMRES or BiCGStab is specified, the adjoint problem requires solving a number of linear problems equal to the number of requested eigenvalues and eigenvectors. If an eigenvalue solver is specified, for which Arnoldi is essentially the only option, then the adjoint problem is solved as a single (but larger) eigenvalue problem.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Note that the phase of an eigenvector is not uniquely determined. Hence, a well-defined cost function constructed from eigenvectors should depend on these in such a way that its value is not affected by changing the phase of those eigenvectors, i.e. the cost function should be 'gauge invariant'. If this is not the case, the cost function is said to be 'gauge dependent', and this can be detected in the resulting adjoint variables for those eigenvectors. The KrylovKit rrule for eigsolve will print a warning if it detects from the incoming adjoint variables that the cost function is gauge dependent. This warning can be suppressed by passing alg_rrule an algorithm with verbosity=-1.","category":"page"},{"location":"man/eig/#Generalized-eigenvalue-problems","page":"Eigenvalue problems","title":"Generalized eigenvalue problems","text":"","category":"section"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Generalized eigenvalues λ and corresponding vectors x of the generalized eigenvalue problem A x = λ B x can be obtained using the method geneigsolve. Currently, there is only one algorithm, which does not require inverses of A or B, but is restricted to symmetric or hermitian generalized eigenvalue problems where the matrix or linear map B is positive definite. Note that this is not reflected in the default values for the keyword arguments issymmetric, ishermitian and isposdef, so that these should be set explicitly in order to comply with this restriction. If A and B are actual instances of AbstractMatrix, the default value for the keyword arguments will try to check these properties explicitly.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"geneigsolve","category":"page"},{"location":"man/eig/#KrylovKit.geneigsolve","page":"Eigenvalue problems","title":"KrylovKit.geneigsolve","text":"geneigsolve((A::AbstractMatrix, B::AbstractMatrix), [howmany = 1, which = :LM,\n                                T = promote_type(eltype(A), eltype(B))]; kwargs...)\ngeneigsolve(f, n::Int, [howmany = 1, which = :LM, T = Float64]; kwargs...)\ngeneigsolve(f, x₀, [howmany = 1, which = :LM]; kwargs...)\n# expert version:\ngeneigsolve(f, x₀, howmany, which, algorithm)\n\nCompute at least howmany generalized eigenvalues λ and generalized eigenvectors x of the form (A - λB)x = 0, where A and B are either instances of AbstractMatrix, or some function that implements the matrix vector product. In case functions are used, one could either specify the action of A and B via a tuple of two functions (or a function and an AbstractMatrix), or one could use a single function that takes a single argument x and returns two results, corresponding to A*x and B*x. Return the computed eigenvalues, eigenvectors and a ConvergenceInfo structure.\n\nArguments:\n\nThe first argument is either a tuple of two linear maps, so a function or an AbstractMatrix for either of them, representing the action of A and B. Alternatively, a single function can be used that takes a single argument x and returns the equivalent of (A*x, B*x) as result. This latter form is compatible with the do block syntax of Julia. If an AbstractMatrix is used for either A or B, a starting vector x₀ does not need to be provided, it is then chosen as rand(T, size(A,1)) if A is an AbstractMatrix (or similarly if only B is an AbstractMatrix). Here T = promote_type(eltype(A), eltype(B)) if both A and B are instances of AbstractMatrix, or just the eltype of whichever is an AbstractMatrix. If both A and B are encoded more generally as a callable function or method, the best approach is to provide an explicit starting guess x₀. Note that x₀ does not need to be of type AbstractVector, any type that behaves as a vector and supports the required methods (see KrylovKit docs) is accepted. If instead of x₀ an integer n is specified, it is assumed that x₀ is a regular vector and it is initialized to rand(T,n), where the default value of T is Float64, unless specified differently.\n\nThe next arguments are optional, but should typically be specified. howmany specifies how many eigenvalues should be computed; which specifies which eigenvalues should be targeted. Valid specifications of which are given by\n\n:LM: eigenvalues of largest magnitude\n:LR: eigenvalues with largest (most positive) real part\n:SR: eigenvalues with smallest (most negative) real part\n:LI: eigenvalues with largest (most positive) imaginary part, only if T <: Complex\n:SI: eigenvalues with smallest (most negative) imaginary part, only if T <: Complex\nEigSorter(f; rev = false): eigenvalues λ that appear first (or last if rev == true) when sorted by f(λ)\n\nnote: Note about selecting `which` eigenvalues\nKrylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. Even with ClosestTo, no shift and invert is performed. This is useful if, e.g., you know the spectrum to be within the unit circle in the complex plane, and want to target the eigenvalues closest to the value λ = 1.\n\nThe argument T acts as a hint in which Number type the computation should be performed, but is not restrictive. If the linear map automatically produces complex values, complex arithmetic will be used even though T<:Real was specified.\n\nReturn values:\n\nThe return value is always of the form vals, vecs, info = geneigsolve(...) with\n\nvals: a Vector containing the eigenvalues, of length at least howmany, but could be longer if more eigenvalues were converged at the same cost.\nvecs: a Vector of corresponding eigenvectors, of the same length as vals. Note that eigenvectors are not returned as a matrix, as the linear map could act on any custom Julia type with vector like behavior, i.e. the elements of the list vecs are objects that are typically similar to the starting guess x₀, up to a possibly different eltype. When the linear map is a simple AbstractMatrix, vecs will be Vector{Vector{<:Number}}.\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: indicates how many eigenvalues and eigenvectors were actually converged to the specified tolerance tol (see below under keyword arguments)\ninfo.residual::Vector: a list of the same length as vals containing the residuals info.residual[i] = f(vecs[i]) - vals[i] * vecs[i]\ninfo.normres::Vector{<:Real}: list of the same length as vals containing the norm of the residual info.normres[i] = norm(info.residual[i])\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if not all requested eigenvalues were converged, so always check if info.converged >= howmany.\n\nKeyword arguments:\n\nKeyword arguments and their default values are given by:\n\nverbosity::Int = 0: verbosity level, i.e. \n0 (suppress all messages)\n1 (only warnings)\n2 (one message with convergence info at the end)\n3 (progress info after every iteration)\ntol::Real: the requested accuracy, relative to the 2-norm of the corresponding eigenvectors, i.e. convergence is achieved if norm((A - λB)x) < tol * norm(x). Because eigenvectors are now normalised such that dot(x, B*x) = 1, norm(x) is not automatically one. If you work in e.g. single precision (Float32), you should definitely change the default value.\nkrylovdim::Integer: the maximum dimension of the Krylov subspace that will be constructed. Note that the dimension of the vector space is not known or checked, e.g. x₀ should not necessarily support the Base.length function. If you know the actual problem dimension is smaller than the default value, it is useful to reduce the value of krylovdim, though in principle this should be detected.\nmaxiter::Integer: the number of times the Krylov subspace can be rebuilt; see below for further details on the algorithms.\north::Orthogonalizer: the orthogonalization method to be used, see Orthogonalizer\nissymmetric::Bool: if both linear maps A and B are symmetric, only meaningful if T<:Real\nishermitian::Bool: if both linear maps A and B are hermitian\nisposdef::Bool: if the linear map B is positive definite\n\nThe default values are given by tol = KrylovDefaults.tol, krylovdim = KrylovDefaults.krylovdim, maxiter = KrylovDefaults.maxiter, orth = KrylovDefaults.orth; see KrylovDefaults for details.\n\nThe default value for the last three parameters depends on the method. If an AbstractMatrix is used, issymmetric, ishermitian and isposdef are checked for that matrix, otherwise the default values are issymmetric = false and ishermitian = T <: Real && issymmetric. When values are provided, no checks will be performed even in the matrix case.\n\nAlgorithm\n\nThe last method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here the algorithm is specified, though currently only GolubYe is available. The Golub-Ye algorithm is an algorithm for solving hermitian (symmetric) generalized eigenvalue problems A x = λ B x with positive definite B, without the need for inverting B. It builds a Krylov subspace of size krylovdim starting from an estimate x by acting with (A - ρ(x) B), where  ρ(x) = dot(x, A*x)/ dot(x, B*x), and employing the Lanczos algorithm. This process is repeated at most maxiter times. In every iteration k>1, the subspace will also be expanded to size krylovdim+1 by adding x_k - x_k-1, which is known as the LOPCG correction and was suggested by Money and Ye. With krylovdim = 2, this algorithm becomes equivalent to LOPCG.\n\nwarning: Restriction to symmetric definite generalized eigenvalue problems\nWhile the only algorithm so far is restricted to symmetric/hermitian generalized eigenvalue problems with positive definite B, this is not reflected in the default values for the keyword arguments issymmetric or ishermitian and isposdef. Make sure to set these to true to understand the implications of using this algorithm.\n\n\n\n\n\n","category":"function"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"Currently, there is rrule and thus no automatic differentiation support for geneigsolve.","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"[1]: For a linear map, the adjoint or pullback required in the reverse-order chain rule coincides","category":"page"},{"location":"man/eig/","page":"Eigenvalue problems","title":"Eigenvalue problems","text":"with its (conjugate) transpose, at least with respect to the standard Euclidean inner product.","category":"page"},{"location":"#KrylovKit.jl","page":"Home","title":"KrylovKit.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package collecting a number of Krylov-based algorithms for linear problems, singular value and eigenvalue problems and the application of functions of linear maps or operators to vectors.","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"KrylovKit.jl accepts general functions or callable objects as linear maps, and general Julia objects with vector like behavior (see below) as vectors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The high level interface of KrylovKit is provided by the following functions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"linsolve: solve linear systems A*x = b\nlssolve: solve least square problems A*x ≈ b\neigsolve: find a few eigenvalues and corresponding eigenvectors of an   eigenvalue problem A*x = λ x\ngeneigsolve: find a few eigenvalues and corresponding vectors of a   generalized eigenvalue problem A*x = λ*B*x\nsvdsolve: find a few singular values and corresponding left and right   singular vectors A*x = σ * y and A'*y = σ*x\nexponentiate: apply the exponential of a linear map to a vector x=exp(t*A)*x₀\nexpintegrator: exponential integrator for a linear non-homogeneous ODE   (generalization of exponentiate)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Furthermore, for specialised use cases, there are functions that can deal with so-called \"real linear maps\", which arise e.g. in the context of differentiable programming:","category":"page"},{"location":"","page":"Home","title":"Home","text":"reallinsolve and realeigsolve","category":"page"},{"location":"#Package-features-and-alternatives","page":"Home","title":"Package features and alternatives","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This section could also be titled \"Why did I create KrylovKit.jl\"?","category":"page"},{"location":"","page":"Home","title":"Home","text":"There are already a fair number of packages with Krylov-based or other iterative methods, such as","category":"page"},{"location":"","page":"Home","title":"Home","text":"IterativeSolvers.jl: part of the   JuliaMath organisation, solves linear systems and least   square problems, eigenvalue and singular value problems\nKrylov.jl: part of the   JuliaSmoothOptimizers organisation, solves   linear systems and least square problems on CPU or GPU for any data type that supports mul!(),   including dense and sparse matrices, and abstract operators such as those defined from   LinearOperators.jl or   LinearMaps.jl.\nKrylovMethods.jl: specific for sparse   matrices\nExpokit.jl: application of the matrix   exponential to a vector\nArnoldiMethod.jl: Implicitly restarted   Arnoldi method for eigenvalues of a general matrix\nJacobiDavidson.jl: Jacobi-Davidson   method for eigenvalues of a general matrix\nExponentialUtilities.jl: Krylov   subspace methods for matrix exponentials and phiv exponential integrator products. It   has specialized methods for subspace caching, time stepping, and error testing which are   essential for use in high order exponential integrators.\nOrdinaryDiffEq.jl:   contains implementations of high order exponential integrators   with adaptive Krylov-subspace calculations for solving semilinear and nonlinear ODEs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Some of these packages have certainly inspired and influenced the development of KrylovKit.jl. However, KrylovKit.jl distinguishes itself from the previous packages in the following ways:","category":"page"},{"location":"","page":"Home","title":"Home","text":"KrylovKit accepts general functions to represent the linear map or operator that defines  the problem, without having to wrap them in a  LinearMap or  LinearOperator type.  Of course, subtypes of AbstractMatrix are also supported. If the linear map (always  the first argument) is a subtype of AbstractMatrix, matrix vector multiplication is  used, otherwise it is applied as a function call.\nKrylovKit does not assume that the vectors involved in the problem are actual subtypes  of AbstractVector. Any Julia object that behaves as a vector is supported, so in  particular higher-dimensional arrays or any custom user type that supports the  interface as defined in VectorInterface.jl.  Aside from arrays filled with scalar entries, this includes tuples, named tuples, and  arbitrarily nested combinations of tuples and arrays. Furthermore, CuArray objects  are fully supported as vectors, so that the application of the linear operator on the  vector can be executed on a GPU. The computations performed within the Krylov subspace,  such as diagonalising the projected matrix, are however always performed on the CPU.\nSince version 0.8, KrylovKit.jl supports reverse-mode AD by defining ChainRulesCore.rrule   definitions for the most common functionality (linsolve, eigsolve, svdsolve).  Hence, reverse mode AD engines that are compatible with the ChainRules  ecosystem will be able to benefit from an optimized implementation of the adjoint  of these functions. The rrule definitions for the remaining functionality   (geneigsolve and expintegrator, of which exponentiate is a special case) will be  added at a later stage. There is a dedicated documentation page on how to configure these  rrules, as they also require to solve large-scale linear or eigenvalue problems.","category":"page"},{"location":"#Current-functionality","page":"Home","title":"Current functionality","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following algorithms are currently implemented","category":"page"},{"location":"","page":"Home","title":"Home","text":"linsolve: CG, GMRES, BiCGStab\neigsolve: a Krylov-Schur algorithm (i.e. with tick restarts) for extremal eigenvalues   of normal (i.e. not generalized) eigenvalue problems, corresponding to   Lanczos for real symmetric or complex hermitian linear maps, and to   Arnoldi for general linear maps.\ngeneigsolve: an customized implementation of the inverse-free algorithm of Golub and   Ye for symmetric / hermitian generalized eigenvalue problems with positive definite   matrix B in the right hand side of the generalized eigenvalue problem A v = B v λ.   The Matlab implementation was described by Money and Ye and is known as EIGIFP; in   particular it extends the Krylov subspace with a vector corresponding to the step   between the current and previous estimate, analogous to the locally optimal   preconditioned conjugate gradient method (LOPCG). In particular, with Krylov dimension   2, it becomes equivalent to the latter.\nsvdsolve: finding largest singular values based on Golub-Kahan-Lanczos   bidiagonalization (see GKL)\nexponentiate: a Lanczos or Arnoldi based algorithm for the action   of the exponential of linear map.\nexpintegrator: exponential integrator   for a linear non-homogeneous ODE, computes a linear combination of the ϕⱼ functions which generalize ϕ₀(z) = exp(z).","category":"page"},{"location":"#Future-functionality?","page":"Home","title":"Future functionality?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here follows a wish list / to-do list for the future. Any help is welcomed and appreciated.","category":"page"},{"location":"","page":"Home","title":"Home","text":"More algorithms, including biorthogonal methods:\nfor linsolve: L-GMRES, MINRES, BiCG, IDR(s), ...\nfor lssolve: LSQR, ...\nfor eigsolve: BiLanczos, Jacobi-Davidson JDQR/JDQZ, subspace iteration (?), ...\nfor geneigsolve: trace minimization, ...\nSupport both in-place / mutating and out-of-place functions as linear maps\nReuse memory for storing vectors when restarting algorithms (related to previous)\nSupport non-BLAS scalar types using GeneralLinearAlgebra.jl and GeneralSchur.jl\nNonlinear eigenvalue problems\nPreconditioners\nRefined Ritz vectors, Harmonic Ritz values and vectors\nBlock versions of the algorithms\nMore relevant matrix functions","category":"page"},{"location":"","page":"Home","title":"Home","text":"Partially done:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Improved efficiency for the specific case where x is Vector (i.e. BLAS level 2   operations): any vector v::AbstractArray which has IndexStyle(v) == IndexLinear()   now benefits from a multithreaded (use export JULIA_NUM_THREADS = x with x the   number of threads you want to use) implementation that resembles BLAS level 2 style for   the vector operations, provided ClassicalGramSchmidt(), ClassicalGramSchmidt2() or   ClassicalGramSchmidtIR() is chosen as orthogonalization routine.","category":"page"},{"location":"man/linear/#Linear-problems","page":"Linear problems","title":"Linear problems","text":"","category":"section"},{"location":"man/linear/#Linear-systems","page":"Linear problems","title":"Linear systems","text":"","category":"section"},{"location":"man/linear/","page":"Linear problems","title":"Linear problems","text":"Linear systems are of the form A*x=b where A should be a linear map that has the same type of output as input, i.e. the solution x should be of the same type as the right hand side b. They can be solved using the function linsolve:","category":"page"},{"location":"man/linear/","page":"Linear problems","title":"Linear problems","text":"linsolve","category":"page"},{"location":"man/linear/#KrylovKit.linsolve","page":"Linear problems","title":"KrylovKit.linsolve","text":"linsolve(A::AbstractMatrix, b::AbstractVector, [x₀, a₀::Number = 0, a₁::Number = 1]; kwargs...)\nlinsolve(f, b, [x₀, a₀::Number = 0, a₁::Number = 1]; kwargs...)\n# expert version:\nlinsolve(f, b, x₀, algorithm, [a₀::Number = 0, a₁::Number = 1]; alg_rrule=algorithm)\n\nCompute a solution x to the linear system (a₀ + a₁ * A)*x = b or a₀ * x + a₁ * f(x) = b, possibly using a starting guess x₀. Return the approximate solution x and a ConvergenceInfo structure.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object. If no initial guess is specified, it is chosen as (zero(a₀)*zero(a₁))*b which should generate an object similar to b but initialized with zeros. The numbers a₀ and a₁ are optional arguments; they are applied implicitly, i.e. they do not contribute the computation time of applying the linear map or to the number of operations on vectors of type x and b.\n\nReturn values:\n\nThe return value is always of the form x, info = linsolve(...) with\n\nx: the approximate solution to the problem, similar type as the right hand side b but possibly with a different scalartype\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: takes value 0 or 1 depending on whether the solution was converged up to the requested tolerance\ninfo.residual: residual b - f(x) of the approximate solution x\ninfo.normres::Real: norm of the residual, i.e. norm(info.residual)\ninfo.numops::Int: total number of times that the linear map was applied, i.e. the number of times that f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if no converged solution was found, so always check if info.converged == 1.\n\nKeyword arguments:\n\nKeyword arguments are given by:\n\nverbosity::Int = 0: verbosity level, i.e. \n0 (suppress all messages)\n1 (only warnings)\n2 (information at the beginning and end)\n3 (progress info after every iteration)\natol::Real: the requested accuracy, i.e. absolute tolerance, on the norm of the residual.\nrtol::Real: the requested accuracy on the norm of the residual, relative to the norm of the right hand side b.\ntol::Real: the requested accuracy on the norm of the residual that is actually used by the algorithm; it defaults to max(atol, rtol*norm(b)). So either use atol and rtol or directly use tol (in which case the value of atol and rtol will be ignored).\nkrylovdim::Integer: the maximum dimension of the Krylov subspace that will be constructed.\nmaxiter::Integer: the number of times the Krylov subspace can be rebuilt; see below for further details on the algorithms.\north::Orthogonalizer: the orthogonalization method to be used, see Orthogonalizer\nissymmetric::Bool: if the linear map is symmetric, only meaningful if T<:Real\nishermitian::Bool: if the linear map is hermitian\nisposdef::Bool: if the linear map is positive definite\n\nThe default values are given by atol = KrylovDefaults.tol[], rtol = KrylovDefaults.tol[], tol = max(atol, rtol*norm(b)), krylovdim = KrylovDefaults.krylovdim[], maxiter = KrylovDefaults.maxiter[], orth = KrylovDefaults.orth; see KrylovDefaults for details.\n\nThe default value for the last three parameters depends on the method. If an AbstractMatrix is used, issymmetric, ishermitian and isposdef are checked for that matrix, ortherwise the default values are issymmetric = false, ishermitian = T <: Real && issymmetric and isposdef = false.\n\nThe final keyword argument alg_rrule is relevant only when linsolve is used in a setting where reverse-mode automatic differentation will be used. A custom ChainRulesCore.rrule is defined for linsolve, which can be evaluated using different algorithms that can be specified via alg_rrule. As the pullback of linsolve involves solving a linear system with the (Hermitian) adjoint of the linear map, the default value is to use the same algorithm. This keyword argument should only be used when this default choice is failing or not performing efficiently. Check the documentation for more information on the possible values for alg_rrule and their implications on the algorithm being used.\n\nAlgorithms\n\nThe final (expert) method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here, one specifies the algorithm explicitly. Currently, only CG, GMRES, BiCGStab and LSMR are implemented, where CG is chosen if isposdef == true and GMRES is chosen otherwise. Note that in standard GMRES terminology, our parameter krylovdim is referred to as the restart parameter, and our maxiter parameter counts the number of outer iterations, i.e. restart cycles. In CG, the Krylov subspace is only implicit because short recurrence relations are being used, and therefore no restarts are required. Therefore, we pass krylovdim*maxiter as the maximal number of CG iterations that can be used by the CG algorithm.\n\n\n\n\n\n","category":"function"},{"location":"man/linear/#Automatic-differentation","page":"Linear problems","title":"Automatic differentation","text":"","category":"section"},{"location":"man/linear/","page":"Linear problems","title":"Linear problems","text":"The linsolve routine can be used in conjunction with reverse-mode automatic differentiation, using AD engines that are compatible with the ChainRules ecosystem. The adjoint problem of a linear problem is again a linear problem, that requires the adjoint[1] of the linear map. If the linear map is an AbstractMatrix instance, its adjoint will be used in the rrule. If the linear map is implemented as a function f, then the AD engine itself is used to compute the corresponding adjoint via ChainRulesCore.rrule_via_ad(config, f, x). The specific base point x at which this adjoint is computed should not affect the result if f properly represents a linear map. Furthermore, the linsolve output is only affected by the linear map argument and the right hand side argument b (from a theoretical perspective, the starting vector and algorithm parameters should have no effect), so that these two arguments are where the adjoint  variables need to be propagated to and have a nonzero effect.","category":"page"},{"location":"man/linear/","page":"Linear problems","title":"Linear problems","text":"The adjoint linear problem (also referred to as cotangent problem) is by default solved using the same algorithms as the primal problem. However, the rrule can be customized to use a different Krylov algorithm, by specifying the alg_rrule keyword argument. Its value can take any of the values as the algorithm argument in linsolve.","category":"page"},{"location":"man/linear/","page":"Linear problems","title":"Linear problems","text":"[1]: For a linear map, the adjoint or pullback required in the reverse-order chain rule coincides","category":"page"},{"location":"man/linear/","page":"Linear problems","title":"Linear problems","text":"with its (conjugate) transpose, at least with respect to the standard Euclidean inner product.","category":"page"},{"location":"man/reallinear/#Real-linear-maps","page":"Real linear maps","title":"Real linear maps","text":"","category":"section"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"A map f V to V from some vector space V to itself is said to be a real linear map if it satisfies f(alpha x + beta y) = alpha f(x) + beta f(y) for all x y in V and all alpha beta in mathbbR. When V is itself a real vector space, this is just the natural concept of a linear map. However, this definition can be used even if x and y are naturally represented using complex numbers and arithmetic and also admit complex linear combinations, i.e. if V is a complex vector space.","category":"page"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"Such real linear maps arise whenever f(x) involves calling conj(x), and are for example obtained in the context of Jacobians (pullbacks) of complex valued functions that are not holomorphic.","category":"page"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"To deal with real linear maps, one should reinterpret V as a real vector space, by restricting the possible linear combinations to those with real scalar coefficients, and by using the real part of the inner product. When the vectors are explictly represented as some AbstractVector{Complex{T}}, this could be obtained by explicitly splitting them in their real and imaginary parts and stacking those into AbstractVector{T} objects with twice the original length.","category":"page"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"However, KrylovKit.jl admits a different approach, where the original representation of vectors is kept, and the inner product is simply replaced by its real part. KrylovKit.jl offers specific methods for solving linear systems and eigenvalue systems in this way. For linear problems, this is implemented using reallinsolve:","category":"page"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"reallinsolve","category":"page"},{"location":"man/reallinear/#KrylovKit.reallinsolve","page":"Real linear maps","title":"KrylovKit.reallinsolve","text":"reallinsolve(f, b, x₀, algorithm, [a₀::Real = 0, a₁::Real = 1]; alg_rrule=algorithm)\n\nCompute a solution x to the linear system a₀ * x + a₁ * f(x) = b, using a starting guess x₀, where f represents a real linear map. Return the approximate solution x and a ConvergenceInfo structure.\n\nnote: Note about real linear maps\nA function f is said to implement a real linear map if it satisfies  f(add(x,y)) = add(f(x), f(y) and f(scale(x, α)) = scale(f(x), α) for vectors x and y and scalars α::Real. Note that this is possible even when the vectors are represented using complex arithmetic. For example, the map f=x-> x + conj(x) represents a real linear map that is not (complex) linear, as it does not satisfy f(scale(x, α)) = scale(f(x), α) for complex scalars α. Note that complex linear maps are always real linear maps and thus can be used in this context, though in that case linsolve and reallinsolve target the same solution. However, they still compute that solution using different arithmetic, and in that case linsolve might be more efficient.To interpret the vectors x and y as elements from a real vector space, the standard inner product defined on them will be replaced with real(inner(x,y)). This has no effect if the vectors x and y were represented using real arithmetic to begin with, and allows to seemlessly use complex vectors as well.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object. The real numbers a₀ and a₁ are optional arguments; they are applied  implicitly,  i.e. they do not contribute the computation time of applying the linear map or to the  number of operations on vectors of type x and b.\n\nReturn values:\n\nThe return value is always of the form x, info = linsolve(...) with\n\nx: the approximate solution to the problem, similar type as the right hand side b but possibly with a different scalartype\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: takes value 0 or 1 depending on whether the solution was converged up to the requested tolerance\ninfo.residual: residual b - f(x) of the approximate solution x\ninfo.normres::Real: norm of the residual, i.e. norm(info.residual)\ninfo.numops::Int: total number of times that the linear map was applied, i.e. the number of times that f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if no converged solution was found, so always check if info.converged == 1.\n\nAlgorithms\n\nThe final (expert) method, without default values and keyword arguments, is the one that is finally called, and can also be used directly. Here, one specifies the algorithm explicitly. Currently, only CG, GMRES and BiCGStab are implemented, where CG is chosen if isposdef == true and GMRES is chosen otherwise. Note that in standard GMRES terminology, our parameter krylovdim is referred to as the restart parameter, and our maxiter parameter counts the number of outer iterations, i.e. restart cycles. In CG, the Krylov subspace is only implicit because short recurrence relations are being used, and therefore no restarts are required. Therefore, we pass krylovdim*maxiter as the maximal number of CG iterations that can be used by the CG algorithm.\n\n\n\n\n\n","category":"function"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"In the case of eigenvalue systems, a similar method realeigsolve is available. In this context, only real eigenvalues are meaningful, as the corresponding eigenvectors should be built from real linear combinations of the vectors that span the (real) Krylov subspace. This approach can also be applied to linear maps on vectors that were naturally real to begin with, if it is guaranteed that the targetted eigenvalues are real. In that case, also the associated eigenvectors will be returned using only real arithmic. This is contrast with eigsolve, which will always turn to complex arithmetic if the linear map is real but not symmetric. An error will be thrown if complex eigenvalues are encountered within the targetted set.","category":"page"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"realeigsolve","category":"page"},{"location":"man/reallinear/#KrylovKit.realeigsolve","page":"Real linear maps","title":"KrylovKit.realeigsolve","text":"# expert version:\nrealeigsolve(f, x₀, howmany, which, algorithm; alg_rrule=algorithm)\n\nCompute the first howmany eigenvalues (according to the order specified by which) from the real linear map encoded in the matrix A or by the function f, if it can be guaranteed that these eigenvalues (and thus their associated eigenvectors) are real. An error will be thrown if there are complex eigenvalues within the first howmany eigenvalues.\n\nReturn eigenvalues, eigenvectors and a ConvergenceInfo structure.\n\nnote: Note about real linear maps\nA function f is said to implement a real linear map if it satisfies  f(add(x,y)) = add(f(x), f(y) and f(scale(x, α)) = scale(f(x), α) for vectors x and y and scalars α::Real. Note that this is possible even when the vectors are represented using complex arithmetic. For example, the map f=x-> x + conj(x) represents a real linear map that is not (complex) linear, as it does not satisfy f(scale(x, α)) = scale(f(x), α) for complex scalars α. Note that complex linear maps are always real linear maps and thus can be used in this context, if looking specifically for real eigenvalues that they may have.To interpret the vectors x and y as elements from a real vector space, the standard inner product defined on them will be replaced with real(inner(x,y)). This has no effect if the vectors x and y were represented using real arithmetic to begin with, and allows to seemlessly use complex vectors as well.\n\nArguments:\n\nThe linear map can be an AbstractMatrix (dense or sparse) or a general function or callable object. A starting vector x₀ needs to be provided. Note that x₀ does not need to be of type AbstractVector; any type that behaves as a vector and supports the required interface (see KrylovKit docs) is accepted.\n\nThe argument howmany specifies how many eigenvalues should be computed; which specifies which eigenvalues should be targeted. Valid specifications of which for real problems are given by\n\n:LM: eigenvalues of largest magnitude\n:LR: eigenvalues with largest (most positive) real part\n:SR: eigenvalues with smallest (most negative) real part\nEigSorter(f; rev = false): eigenvalues λ that appear first (or last if rev == true) when sorted by f(λ)\n\nnote: Note about selecting `which` eigenvalues\nKrylov methods work well for extremal eigenvalues, i.e. eigenvalues on the periphery of the spectrum of the linear map. All of the valid Symbols for which have this property, but could also be specified using EigSorter, e.g. :LM is equivalent to Eigsorter(abs; rev = true). Note that smallest magnitude sorting is obtained using e.g. EigSorter(abs; rev = false), but since no (shift-and)-invert is used, this will only be successful if you somehow know that eigenvalues close to zero are also close to the periphery of the spectrum.\n\nwarning: Degenerate eigenvalues\nFrom a theoretical point of view, Krylov methods can at most find a single eigenvector associated with a targetted eigenvalue, even if the latter is degenerate. In the case of a degenerate eigenvalue, the specific eigenvector that is returned is determined by the starting vector x₀. For large problems, this turns out to be less of an issue in practice, as often a second linearly independent eigenvector is generated out of the numerical noise resulting from the orthogonalisation steps in the Lanczos or Arnoldi iteration. Nonetheless, it is important to take this into account and to try not to depend on this potentially fragile behaviour, especially for smaller problems.\n\nThe algorithm argument currently only supports an instance of Arnoldi, which is where the parameters of the Krylov method (such as Krylov dimension and maximum number of iterations) can be specified. Since realeigsolve is less commonly used as eigsolve, it only supports this expert mode call syntax and no convenient keyword interface is currently available.\n\nThe keyword argument alg_rrule can be used to specify an algorithm to be used for computing the pullback of realeigsolve in the context of reverse-mode automatic differentation.\n\nReturn values:\n\nThe return value is always of the form vals, vecs, info = eigsolve(...) with\n\nvals: a Vector containing the eigenvalues, of length at least howmany, but could be longer if more eigenvalues were converged at the same cost. Eigenvalues will be real, an ArgumentError will be thrown if the first howmany eigenvalues ordered according to which of the linear map are not all real.\nvecs: a Vector of corresponding eigenvectors, of the same length as vals. Note that eigenvectors are not returned as a matrix, as the linear map could act on any custom Julia type with vector like behavior, i.e. the elements of the list vecs are objects that are typically similar to the starting guess x₀. For a real problem with real eigenvalues, also the eigenvectors will be real and no complex arithmetic is used anywhere.\ninfo: an object of type [ConvergenceInfo], which has the following fields\ninfo.converged::Int: indicates how many eigenvalues and eigenvectors were actually converged to the specified tolerance tol (see below under keyword arguments)\ninfo.residual::Vector: a list of the same length as vals containing the residuals info.residual[i] = f(vecs[i]) - vals[i] * vecs[i]\ninfo.normres::Vector{<:Real}: list of the same length as vals containing the norm of the residual info.normres[i] = norm(info.residual[i])\ninfo.numops::Int: number of times the linear map was applied, i.e. number of times f was called, or a vector was multiplied with A\ninfo.numiter::Int: number of times the Krylov subspace was restarted (see below)\n\nwarning: Check for convergence\nNo warning is printed if not all requested eigenvalues were converged, so always check if info.converged >= howmany.\n\n\n\n\n\n","category":"function"},{"location":"man/reallinear/","page":"Real linear maps","title":"Real linear maps","text":"Note that both reallinsolve and realeigsolve currently only exist with the \"expert\" mode interface, where the user has to manually specify the underlying Krylov algorithm and its parameters, i.e. GMRES or BiCGStab for reallinsolve and Arnoldi for realeigsolve.","category":"page"}]
}
